{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "\n",
    "### A regular expression is simply a sequence of characters that define a pattern.\n",
    "\n",
    "- Everything in regex is a character. Even .\n",
    "\n",
    "- Unicode characters can be used to match any international text\n",
    "- Most patterns use normal ASCII (letters, digits, punctuation and keyboard symbols like \\$@%#!.)\n",
    "\n",
    "### Regex does a lot with less – write a few characters to do something that could have taken many lines to code\n",
    "\n",
    "### Applications: Scraping, Parsing, Validating Phone Email etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"abcdef\"\n",
    "text2 = \"zabcdef\"\n",
    "text3 = \"123abc\"\n",
    "text4 = \"Hi abc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.match method\n",
    "\n",
    "- If zero or more characters at the BEGINNING of string match the regular expression pattern: return a corresponding MatchObject instance.\n",
    "- Return None if the string does not match the pattern; note that this is different from a zero-length match.\n",
    "- If you want to locate a match anywhere in string, use search() instead.\n",
    "\n",
    "\n",
    "## re.search method\n",
    "\n",
    "- while re.match checks for a match only at the BEGINNING of the string, re.search() checks for a match ANYWHERE in the string.\n",
    "\n",
    "\n",
    "### both search and match can take an extra argument: re.MULTILINE simply tells our method to search on multiple lines that have been separated by the new line space character if any.\n",
    "\n",
    "### The Match object that both functions return stores details about the part of the string matched by the regular expression pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(\"abc\",text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to show the text itself:\n",
    "# The group() is a match object method that returns an entire match.\n",
    "# If not, it returns an AttributeError for NoneType, which mean there was no match.\n",
    "re.match(\"abc\",text1).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"abc\",text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"abc\",text1).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "re.match(\"abc\",text2)\n",
    "print(type(re.match(\"abc\",text2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-119b82972a83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"abc\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "re.match(\"abc\",text2).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(1, 4), match='abc'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"abc\",text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(\"abc\",text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"abc\",text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(\"abc\",text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"abc\",text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.match, re.search Usage in if statements\n",
    "## Since None evaluates to False, you can easily use re.search() in an if statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid email entered\n"
     ]
    }
   ],
   "source": [
    "text5 = \"blablablagmail.com\"\n",
    "if re.search(\"@\",text5):\n",
    "    print(\"email test for @ ok\")\n",
    "else: print(\"Invalid email entered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundaries and Anchors\n",
    "\n",
    "### ^ and \\$ are boundaries or anchors.\n",
    "### ^ marks the start, while \\$ marks the end of a regular expression.\n",
    "\n",
    "### However, when used in square brackets [^ ... ] it means not.\n",
    "\n",
    "### For example, [^\\s$] or just [^\\s] will tell regex to match anything that is not a whitespace character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if an id number starts with any digit,\n",
    "# also lets check that we have no other symbols rather than numbers\n",
    "\n",
    "id1 = \"200478193\"\n",
    "\n",
    "id2 = \"a200478193\"\n",
    "\n",
    "id3 = \"20047@8193\"\n",
    "\n",
    "id4 = \" 200478193\" # space at the beggining of the string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 1), match='2'>\n"
     ]
    }
   ],
   "source": [
    "# using match\n",
    "# ^ for start of string. [0-9] for digits\n",
    "print(re.match(r\"^[0-9]\",id1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r\"^[0-9]\",id2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 1), match='2'>\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r\"^[0-9]\",id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r\"^[0-9]\",id4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 1), match='2'>\n"
     ]
    }
   ],
   "source": [
    "# using search\n",
    "print(re.search(r\"^[0-9]\",id1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# note that now also search will return None because we require ^\n",
    "print(re.search(r\"^[0-9]\",id2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 1), match='2'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search(r\"^[0-9]\",id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search(r\"^[0-9]\",id4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200478193\n",
      "bla bla\n",
      "7\n",
      "bla bla \n",
      "8\n",
      " bla bla\n",
      "8\n",
      "blabla\n",
      "6\n",
      "blabla\n"
     ]
    }
   ],
   "source": [
    "# trimming whitespace\n",
    "\n",
    "print(id4.strip())\n",
    "\n",
    "my_string = \" bla bla \"\n",
    "\n",
    "# left and right (not middle)\n",
    "print(my_string.strip())\n",
    "print(len(my_string.strip()))\n",
    "\n",
    "# left\n",
    "print(my_string.lstrip())\n",
    "print(len(my_string.lstrip()))\n",
    "\n",
    "# right\n",
    "print(my_string.rstrip())\n",
    "print(len(my_string.rstrip()))\n",
    "\n",
    "# all including middle\n",
    "print(my_string.replace(\" \",\"\"))\n",
    "print(len(my_string.replace(\" \",\"\")))\n",
    "\n",
    "# another method for all including middle\n",
    "pattern = r\"\\s\"\n",
    "print(re.sub(pattern,\"\",my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_id_is_correct(id):\n",
    "    \n",
    "    id = id.strip() # remove whitespace\n",
    "    \n",
    "    # check that begins with number, or use match\n",
    "    # and also check that it doesn't contain chars that are not digits\n",
    "    \n",
    "    if re.search(r\"^\\d\",id) and not re.search(r\"\\D\",id):\n",
    "        \n",
    "        print(\"id is ok\")\n",
    "        \n",
    "    else: print(\"id input error\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id is ok\n",
      "id input error\n",
      "id input error\n",
      "id is ok\n"
     ]
    }
   ],
   "source": [
    "check_id_is_correct(id1)\n",
    "check_id_is_correct(id2)\n",
    "check_id_is_correct(id3)\n",
    "check_id_is_correct(id4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'007'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_at_least_one_digit(string):\n",
    "    \"\"\"This function returns at least one matching digit.\"\"\"\n",
    "    pattern = re.compile(r\"\\d{1,}\") # this is the same as r\"\\d+\"\n",
    "    result = pattern.search(string)\n",
    "    if result:\n",
    "        return  result.group()\n",
    "    return None\n",
    "\n",
    "# Call our function, passing in our string\n",
    "return_at_least_one_digit(\"James Bond 007\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"I like to learn about python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 28), match='I like to learn about python'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "re.search(r\".*\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 1), match='I'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"\\w\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 1), match='I'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r\"\\w\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 1), match='I'>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"\\w+\", text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.findall\n",
    "\n",
    "### returns a list of all matches found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " 't',\n",
       " 'o',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " 'a',\n",
       " 'b',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " 'p',\n",
       " 'y',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\w\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'to', 'learn', 'about', 'python']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\w+\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r\"\\s\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(2, 3), match='l'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"[a-z]\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(2, 6), match='like'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"[a-z]+\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'to', 'learn', 'about', 'python']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[a-z]+\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'to', 'learn', 'about', 'python']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[a-zA-Z]+\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pattern = re.compile(r\"^<html>\")\n",
    "result = pattern.search(\"<html></html>\")\n",
    "print(result.group())\n",
    "\n",
    "pattern = re.compile(r\"</html>$\")\n",
    "result = pattern.search(\"<html></html>\")\n",
    "print(result.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"John Doe\n",
    "Jane Doe\n",
    "Jin Du\n",
    "Chin Doe\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John Doe', 'Jane Doe', 'Jin Du', 'Chin Doe']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = re.split(r\"\\n+\", text)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, how are you\"\n",
    "\n",
    "results = re.split(r\"\\s\",text)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', '', '', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello,   how are you\" # with 3 spaces\n",
    "\n",
    "results = re.split(r\"\\s\",text)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = re.split(r\"\\s+\",text)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'python']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"python is very nice, I think that python is very usefull\"\n",
    "\n",
    "re.findall(\"python\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello,   how are you\"\n",
    "\n",
    "re.findall(r'\\S+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\w+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[a-zA-Z]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a string method, not re\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o', 'h', 'o', 'w', 'a', 'r', 'e', 'y', 'o', 'u']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\w', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o', 'h', 'o', 'w', 'a', 'r', 'e', 'y', 'o', 'u']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[a-zA-Z]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'how', 'are', 'you']\n",
      "Hello, how are you\n"
     ]
    }
   ],
   "source": [
    "print(text.split())\n",
    "print(\" \".join(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We met __ girl and __ boys'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that sub runs through the whole string and is true for any match\n",
    "text = \"We met 1 girl and 22 boys\"\n",
    "pattern = r\"[0-9]+\"\n",
    "result = re.sub(pattern,\"__\", text)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We met __ girl and ____ boys'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 and 2 are treated separatly since they match twice\n",
    "pattern = r\"[0-9]\"\n",
    "result = re.sub(pattern,\"__\", text)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '22']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"[0-9]+\"\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '2']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"[0-9]\"\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  1   22 '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all alphabetic words\n",
    "pattern = r\"[a-zA-Z]\\w+\"\n",
    "result = re.sub(pattern,\"\", text)\n",
    "result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More examples with all the functions we learned so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello my friend! How have you been? I was looking for you. Why didn't you answer my call 5 hours ago?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['Hello my friend', ' How have you been', ' I was looking for you', \" Why didn't you answer my call 5 hours ago\", '']\n"
     ]
    }
   ],
   "source": [
    "sentence_ending = r\"[.!?]\"\n",
    "\n",
    "result = re.split(sentence_ending, text)\n",
    "\n",
    "print((len(result)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['Hello', 'How', 'Why']\n"
     ]
    }
   ],
   "source": [
    "# find all capitalized words\n",
    "\n",
    "sentence_ending = r\"[A-Z]\\w+\"\n",
    "\n",
    "result = re.findall(sentence_ending, text)\n",
    "\n",
    "print((len(result)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['5']\n"
     ]
    }
   ],
   "source": [
    "# find all digits\n",
    "\n",
    "sentence_ending = r\"\\d\"\n",
    "\n",
    "result = re.findall(sentence_ending, text)\n",
    "\n",
    "print((len(result)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dasdasdsafsvvfd gvdfvg dfvgd'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"dasdasdsafs[image : image name : image]vvfd gvdfvg dfvgd\"\n",
    "\n",
    "re.sub(r'\\[image :.*: image\\]', r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finding', 'dory']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_list(string):\n",
    "    \"\"\"This function finds all the words in a given string.\"\"\"\n",
    "    result_list = re.findall(r\"\\w+\", string)\n",
    "    return result_list\n",
    "\n",
    "# Call finder function, passing in the string argument\n",
    "get_words_list(\"finding dory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['130000', '110000', '10000']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "salaries = \"130000   110000   10000   1000   300\"\n",
    "\n",
    "re.findall(r\"\\d{5,6}\", salaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brown'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pattern = r'\\w+(?=\\sfox)'\n",
    "result = re.search(pattern,\"The quick brown fox\")\n",
    "result.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Believe', 'better']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pattern = r'\\w+(?=,)'\n",
    "result = re.findall(pattern,\"Believe, you must do better, trust me\") \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['better']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pattern = r'\\w+(?=,)'\n",
    "result = re.findall(pattern,\"Believe , you must do better, trust me\") \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String tokenization\n",
    "### transforming a string into tokens: smaller chunks\n",
    "- one pre-processing step prior to NLP\n",
    "- patterns are used: can split to sentences, words, hashtags from a tweet etc.\n",
    "- regular expressions are usefull\n",
    "- Tokenization can be done using nltk (natural language toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk tokenizers:\n",
    "- word_tokenize\n",
    "- sent_tokenize\n",
    "- regexp_tokenize - tokenize a string or doc based on regular expression\n",
    "- TweetTokenizer - separate to hashtags, mentions etc\n",
    "### you could build your own..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'my', 'friend', '!']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hello my friend!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'like', 'my', 'friend', \"'s\", 'taste', 'in', 'restaurants']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I don't like my friend's taste in restaurants\")\n",
    "\n",
    "# we can see how tokenization helps us detect relations (n't for negative, 's for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_text = \"\"\"Cryptocurrencies have been all the rage.\n",
    "\n",
    "And it’s no wonder when cryptocurrencies like Ethereum shot up 2,740% … digital cash surged 1,092% … and Bitshares jumped 1,446%.\n",
    "\n",
    "And while more money might be made as the cryptocurrency market grows into an estimated $200 billion industry, there’s a much more lucrative place to make money.\n",
    "\n",
    "An industry set to explode a full 8,000% … surging from $235 billion to $4 trillion in the next four years.\n",
    "\n",
    "That’s why one former hedge fund manager — Paul Mampilly — isn’t recommending a single cryptocurrency (to his 100,000-plus readers).\n",
    "\n",
    "Mampilly: “Cryptocurrencies are in a massive bubble. They are too risky,” Mampilly says. “There’s a new, emerging industry that will sky rocket more than any other investment in history. More than bitcoin, more than marijuana, more than biotech … combined.”\n",
    "\n",
    "One should heed Mampilly’s insight.\n",
    "\n",
    "He made a 76% return during the 2008 crash which is part of the reason Barron’s named his co-managed hedge fund “one of the world’s best.”\n",
    "\n",
    "Mampilly became famous for helping millionaires make millions, but turned legendary when he walked away from Wall Street to help Main Street Americans make the same type of returns.\n",
    "\n",
    "And they love him.\n",
    "\n",
    "Over 100,000 people have already flocked to get his insight as he has helped them rack up as much as 6,220% in total winning gains.\n",
    "\n",
    "One guy, a retired man from Buffalo, wrote that he made $998,000 thanks to Paul’s timely advice … and even sent a copy of his brokerage statement to prove it.\n",
    "\n",
    "But all of that could pale in comparison to this little-known industry set to explode 8,000%.\n",
    "\n",
    "Which is why Mampilly recently posted a new video on his website giving all the details behind this surging industry. Best of all … he reveals the one company you need to buy now so that you can profit in 2018.\n",
    "\n",
    "This is the biggest and boldest investment prediction of Paul’s decorated career.\n",
    "\n",
    "This is the type of recommendation Paul used to give to his billionaire clients. But today, Paul is giving it to Main Street Americans so they can get in on the ground floor. [12/12/2018] [Wednesday]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "20\n",
      "Cryptocurrencies have been all the rage.\n",
      "And it’s no wonder when cryptocurrencies like Ethereum shot up 2,740% … digital cash surged 1,092% … and Bitshares jumped 1,446%.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(crypto_text)\n",
    "\n",
    "print(type(sentences))\n",
    "print(len(sentences))\n",
    "\n",
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "27\n",
      "{'wonder', '%', '2,740', 'shot', 'cash', 'surged', 'And', 'when', 's', 'it', '1,092', 'Bitshares', 'like', '.', '1,446', 'no', 'Ethereum', 'jumped', 'cryptocurrencies', 'digital', 'and', '’', '…', 'up'}\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentences[1])\n",
    "print(type(words))\n",
    "print(len(words))\n",
    "unique_words = set(words)\n",
    "print(set(unique_words))\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776 783\n",
      "bitcoin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bitcoin'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.search(\"bitcoin\",crypto_text)\n",
    "\n",
    "print(result.start(), result.end())\n",
    "\n",
    "print(result.group())\n",
    "\n",
    "crypto_text[result.start():result.end()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we didn't know about the case\n",
    "\n",
    "result = re.search(\"Bitcoin\",crypto_text)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(776, 783), match='bitcoin'>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.search(\"Bitcoin\",crypto_text, re.IGNORECASE)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(2076, 2100), match='[12/12/2018] [Wednesday]'>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the first text in square brackets\n",
    "\n",
    "pattern = r\"\\[.*\\]\"\n",
    "\n",
    "re.search(pattern, crypto_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[12/12/2018] [Wednesday]']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pattern = r\"\\[.*\\]\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[12/12/2018]', '[Wednesday]']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.split(r\"\\s\",result[0])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12/12/2018', 'Wednesday']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove square brackets\n",
    "result = [s.replace('[','').replace(']','') for s in result]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nMampilly:']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"[\\w\\s]+:\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$200', '$235', '$998']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pattern = r\"\\$[^\\]]\\d+\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$200', '$235', '$4', '$998']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pattern = r\"\\$\\d+\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$998,000']\n",
      "['$200', '$235', '$998,000']\n",
      "['$200', '$235', '$4', '$998,000']\n",
      "['$200', '$235', '$4', '$998']\n"
     ]
    }
   ],
   "source": [
    "# some numbers have commas\n",
    "\n",
    "pattern = r\"\\$\\d+[,]\\d+\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# ? is zero or more - that means optional\n",
    "\n",
    "pattern = r\"\\$\\d+[,]?\\d+\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "pattern = r\"(\\$\\d+[,]\\d+|\\$\\d+)\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "pattern = r\"(\\$\\d+|\\$\\d+[,]\\d+)\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "print(result)\n",
    "\n",
    "#r\"\\$?\\d+[,]?\\d+%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2,740%', '1,092%', '1,446%', '8,000%', '76%', '6,220%', '8,000%']\n",
      "['$200', '$235', '$4', '$998,000', '2,740%', '1,092%', '1,446%', '8,000%', '76%', '6,220%', '8,000%']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pattern =  r\"\\$?\\d+[,]?\\d+%\"\n",
    "\n",
    "result = re.findall(pattern, crypto_text)\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "all_numbers_in_crypto_text = [re.findall(r\"(\\$\\d+[,]\\d+|\\$\\d+)\", crypto_text),re.findall(r\"\\$?\\d+[,]?\\d+%\", crypto_text)]\n",
    "\n",
    "all_numbers_in_crypto_text = [item for sublist in all_numbers_in_crypto_text for item in sublist]\n",
    "\n",
    "print(all_numbers_in_crypto_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'John', 'I', 'am', '12', 'years', 'old']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pattern = r\"(\\d+|\\w+)\"\n",
    "\n",
    "result = re.findall(pattern, \"my name is John, I am 12 years old\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 15), match='my name is John'>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"[a-zA-Z0-9 ]+\"\n",
    "\n",
    "result = re.match(pattern, \"my name is John, I am 12 years old\")\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Person #3: Are you here? tell me where? it looks beautiful!\"\n",
    "\n",
    "pattern = r\"\\w+(\\?!)\"\n",
    "\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Person',\n",
       " '#3',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'here',\n",
       " '?',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'where',\n",
       " '?',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'beautiful',\n",
       " '!']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"(\\w+|#\\d|\\?|!)\"\n",
    "\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ArmyNavyGame', '#win']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweet = \"It was my honor to attend today’s #ArmyNavyGame in Philadelphia. A GREAT game played all around by our HEROES. Congratulations @ArmyWP_Football on the win! #win\"\n",
    "\n",
    "pattern = r\"#\\w+\"\n",
    "\n",
    "regexp_tokenize(trump_tweet, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ArmyNavyGame', '#win']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern,trump_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we had a list of tweets:\n",
    "\n",
    "elon_must_tweet = \"More than 5,600 pounds of research, crew supplies and hardware left Earth at 1:16 pm ET to head to humanity's orbiting lab. @SpaceX's #Dragon spacecraft is scheduled to arrive at the @Space_Station on Saturday, Dec. 8. Details: https://go.nasa.gov/2rn9GsI\"\n",
    "\n",
    "tweets = [trump_tweet,elon_must_tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-ee2375822994>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# will not natively work on a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py\u001b[0m in \u001b[0;36mregexp_tokenize\u001b[1;34m(text, pattern, gaps, discard_empty, flags)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m    203\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscard_empty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# will not natively work on a list\n",
    "\n",
    "regexp_tokenize(tweets, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#ArmyNavyGame', '#win'], ['#Dragon']]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we can use a list comprehension\n",
    "\n",
    "[regexp_tokenize(t, pattern) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'was', 'my', 'honor', 'to', 'attend', 'today', '’', 's', '#ArmyNavyGame', 'in', 'Philadelphia', '.', 'A', 'GREAT', 'game', 'played', 'all', 'around', 'by', 'our', 'HEROES', '.', 'Congratulations', '@ArmyWP_Football', 'on', 'the', 'win', '!', '#win'], ['More', 'than', '5,600', 'pounds', 'of', 'research', ',', 'crew', 'supplies', 'and', 'hardware', 'left', 'Earth', 'at', '1:16', 'pm', 'ET', 'to', 'head', 'to', \"humanity's\", 'orbiting', 'lab', '.', '@SpaceX', \"'\", 's', '#Dragon', 'spacecraft', 'is', 'scheduled', 'to', 'arrive', 'at', 'the', '@Space_Station', 'on', 'Saturday', ',', 'Dec', '.', '8', '.', 'Details', ':', 'https://go.nasa.gov/2rn9GsI']]\n"
     ]
    }
   ],
   "source": [
    "# TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "result = [tknzr.tokenize(t) for t in tweets]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "31\n",
      "Cryptocurrencies have been all the rage.\n",
      "\n",
      "0\n",
      "And it’s no wonder when cryptocurrencies like Ethereum shot up 2,740% … digital cash surged 1,092% … and Bitshares jumped 1,446%.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lines = crypto_text.split(\"\\n\")\n",
    "\n",
    "print(type(lines))\n",
    "\n",
    "print(len(lines))\n",
    "\n",
    "print(lines[0])\n",
    "print(lines[1])\n",
    "print(len(lines[1]))\n",
    "print(lines[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# remove blank lines\n",
    "lines = [l for l in lines if len(l)>0]\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'no',\n",
       " 'wonder',\n",
       " 'when',\n",
       " 'cryptocurrencies',\n",
       " 'like',\n",
       " 'Ethereum',\n",
       " 'shot',\n",
       " 'up',\n",
       " '2,740',\n",
       " '%',\n",
       " '…',\n",
       " 'digital',\n",
       " 'cash',\n",
       " 'surged',\n",
       " '1,092',\n",
       " '%',\n",
       " '…',\n",
       " 'and',\n",
       " 'Bitshares',\n",
       " 'jumped',\n",
       " '1,446',\n",
       " '%',\n",
       " '.']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And',\n",
       " 'it',\n",
       " 's',\n",
       " 'no',\n",
       " 'wonder',\n",
       " 'when',\n",
       " 'cryptocurrencies',\n",
       " 'like',\n",
       " 'Ethereum',\n",
       " 'shot',\n",
       " 'up',\n",
       " '2',\n",
       " '740',\n",
       " 'digital',\n",
       " 'cash',\n",
       " 'surged',\n",
       " '1',\n",
       " '092',\n",
       " 'and',\n",
       " 'Bitshares',\n",
       " 'jumped',\n",
       " '1',\n",
       " '446']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(lines[1], r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And it’s no wonder when cryptocurrencies like Ethereum shot up 2,740% … digital cash surged 1,092% … and Bitshares jumped 1,446%.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'And it’s no wonder when cryptocurrencies like Ethereum shot up 2740% … digital cash surged 1092% … and Bitshares jumped 1446%.'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(lines[1])\n",
    "\n",
    "\n",
    "pattern = r\"(?<=\\d)[,\\.](?=\\d)\"\n",
    "\n",
    "test = re.sub(pattern, \"\", lines[1])\n",
    "\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "And it’s no wonder when cryptocurrencies like Ethereum shot up 2740% … digital cash surged 1092% … and Bitshares jumped 1446%.\n"
     ]
    }
   ],
   "source": [
    "lines = [re.sub(pattern, \"\", l) for l in lines]\n",
    "\n",
    "print(len(lines))\n",
    "\n",
    "print(lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'it', 's', 'no', 'wonder', 'when', 'cryptocurrencies', 'like', 'Ethereum', 'shot', 'up', '2740', 'digital', 'cash', 'surged', '1092', 'and', 'Bitshares', 'jumped', '1446']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_lines = [regexp_tokenize(l, r\"\\w+\") for l in lines]\n",
    "\n",
    "print(tokenized_lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'it', 's', 'no', 'wonder', 'when', 'cryptocurrencies', 'like', 'Ethereum', 'shot', 'up', '2740%', 'digital', 'cash', 'surged', '1092%', 'and', 'Bitshares', 'jumped', '1446%']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_lines = [regexp_tokenize(l, r\"(\\d+%|\\w+)\") for l in lines]\n",
    "\n",
    "print(tokenized_lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 20, 28, 20, 21, 40, 6, 29, 29, 4, 25, 30, 17, 40, 13, 37]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a frequency list of lengths\n",
    "\n",
    "num_words_in_lines = [len(l) for l in tokenized_lines]\n",
    "\n",
    "num_words_in_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 0., 1., 1., 3., 1., 3., 1., 0., 3.]),\n",
       " array([ 4. ,  7.6, 11.2, 14.8, 18.4, 22. , 25.6, 29.2, 32.8, 36.4, 40. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(num_words_in_lines)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Identification (simple)\n",
    "\n",
    "### We will use Term Frequencies (BOW, TF-IDF)\n",
    "\n",
    "### We will use nltk, Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'is', 'in', 'the', 'box', '.', 'The', 'cat', 'likes', 'the', 'box', '.', 'The', 'box', 'is', 'over', 'the', 'cat', '.']\n"
     ]
    }
   ],
   "source": [
    "string = \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "\n",
    "words_list = word_tokenize(string)\n",
    "\n",
    "print(words_list) # list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "counter = Counter(words_list)\n",
    "\n",
    "print(type(counter)) # result is a counter type object, similar to dictionary\n",
    "\n",
    "print(counter)\n",
    "\n",
    "counter.most_common(2) # the most common method return the n most important fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 19),\n",
       " ('to', 14),\n",
       " ('the', 12),\n",
       " ('’', 10),\n",
       " ('s', 9),\n",
       " (',', 9),\n",
       " ('a', 9),\n",
       " ('of', 8),\n",
       " ('%', 7),\n",
       " ('in', 7)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "crypto_text_words = word_tokenize(crypto_text)\n",
    "\n",
    "# convert the token into lowercase\n",
    "\n",
    "crypto_text_words_lower = [t.lower() for t in crypto_text_words]\n",
    "\n",
    "counter = Counter(crypto_text_words)\n",
    "\n",
    "print(type(counter)) \n",
    "\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mampilly', 6),\n",
       " ('industry', 5),\n",
       " ('one', 5),\n",
       " ('paul', 5),\n",
       " ('cryptocurrencies', 3),\n",
       " ('made', 3),\n",
       " ('make', 3),\n",
       " ('street', 3),\n",
       " ('money', 2),\n",
       " ('cryptocurrency', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "crypto_text_words_lower = [w for w in word_tokenize(crypto_text.lower()) if w.isalpha()]\n",
    "\n",
    "crypto_words_lower_nostop = [w for w in crypto_text_words_lower if w not in stopwords.words('english')]\n",
    "\n",
    "print(len(crypto_words_lower_nostop))\n",
    "\n",
    "Counter(crypto_words_lower_nostop).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cryptocurrencies',\n",
       " 'rage',\n",
       " 'wonder',\n",
       " 'cryptocurrencies',\n",
       " 'like',\n",
       " 'ethereum',\n",
       " 'shot',\n",
       " 'digital',\n",
       " 'cash',\n",
       " 'surged',\n",
       " 'bitshares',\n",
       " 'jumped',\n",
       " 'money',\n",
       " 'might',\n",
       " 'made',\n",
       " 'cryptocurrency',\n",
       " 'market',\n",
       " 'grows',\n",
       " 'estimated',\n",
       " 'billion',\n",
       " 'industry',\n",
       " 'much',\n",
       " 'lucrative',\n",
       " 'place',\n",
       " 'make',\n",
       " 'money',\n",
       " 'industry',\n",
       " 'set',\n",
       " 'explode',\n",
       " 'full',\n",
       " 'surging',\n",
       " 'billion',\n",
       " 'trillion',\n",
       " 'next',\n",
       " 'four',\n",
       " 'year',\n",
       " 'one',\n",
       " 'former',\n",
       " 'hedge',\n",
       " 'fund',\n",
       " 'manager',\n",
       " 'paul',\n",
       " 'mampilly',\n",
       " 'recommending',\n",
       " 'single',\n",
       " 'cryptocurrency',\n",
       " 'reader',\n",
       " 'mampilly',\n",
       " 'cryptocurrencies',\n",
       " 'massive',\n",
       " 'bubble',\n",
       " 'risky',\n",
       " 'mampilly',\n",
       " 'say',\n",
       " 'new',\n",
       " 'emerging',\n",
       " 'industry',\n",
       " 'sky',\n",
       " 'rocket',\n",
       " 'investment',\n",
       " 'history',\n",
       " 'bitcoin',\n",
       " 'marijuana',\n",
       " 'biotech',\n",
       " 'one',\n",
       " 'heed',\n",
       " 'mampilly',\n",
       " 'insight',\n",
       " 'made',\n",
       " 'return',\n",
       " 'crash',\n",
       " 'part',\n",
       " 'reason',\n",
       " 'barron',\n",
       " 'named',\n",
       " 'hedge',\n",
       " 'fund',\n",
       " 'one',\n",
       " 'world',\n",
       " 'mampilly',\n",
       " 'became',\n",
       " 'famous',\n",
       " 'helping',\n",
       " 'millionaire',\n",
       " 'make',\n",
       " 'million',\n",
       " 'turned',\n",
       " 'legendary',\n",
       " 'walked',\n",
       " 'away',\n",
       " 'wall',\n",
       " 'street',\n",
       " 'help',\n",
       " 'main',\n",
       " 'street',\n",
       " 'american',\n",
       " 'make',\n",
       " 'type',\n",
       " 'return',\n",
       " 'love',\n",
       " 'people',\n",
       " 'already',\n",
       " 'flocked',\n",
       " 'get',\n",
       " 'insight',\n",
       " 'helped',\n",
       " 'rack',\n",
       " 'much',\n",
       " 'total',\n",
       " 'winning',\n",
       " 'gain',\n",
       " 'one',\n",
       " 'guy',\n",
       " 'retired',\n",
       " 'man',\n",
       " 'buffalo',\n",
       " 'wrote',\n",
       " 'made',\n",
       " 'thanks',\n",
       " 'paul',\n",
       " 'timely',\n",
       " 'advice',\n",
       " 'even',\n",
       " 'sent',\n",
       " 'copy',\n",
       " 'brokerage',\n",
       " 'statement',\n",
       " 'prove',\n",
       " 'could',\n",
       " 'pale',\n",
       " 'comparison',\n",
       " 'industry',\n",
       " 'set',\n",
       " 'explode',\n",
       " 'mampilly',\n",
       " 'recently',\n",
       " 'posted',\n",
       " 'new',\n",
       " 'video',\n",
       " 'website',\n",
       " 'giving',\n",
       " 'detail',\n",
       " 'behind',\n",
       " 'surging',\n",
       " 'industry',\n",
       " 'best',\n",
       " 'reveals',\n",
       " 'one',\n",
       " 'company',\n",
       " 'need',\n",
       " 'buy',\n",
       " 'profit',\n",
       " 'biggest',\n",
       " 'boldest',\n",
       " 'investment',\n",
       " 'prediction',\n",
       " 'paul',\n",
       " 'decorated',\n",
       " 'career',\n",
       " 'type',\n",
       " 'recommendation',\n",
       " 'paul',\n",
       " 'used',\n",
       " 'give',\n",
       " 'billionaire',\n",
       " 'client',\n",
       " 'today',\n",
       " 'paul',\n",
       " 'giving',\n",
       " 'main',\n",
       " 'street',\n",
       " 'american',\n",
       " 'get',\n",
       " 'ground',\n",
       " 'floor',\n",
       " 'wednesday']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in crypto_words_lower_nostop]\n",
    "\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cryptocurrencies',\n",
       " 'rage',\n",
       " 'wonder',\n",
       " 'cryptocurrencies',\n",
       " 'like',\n",
       " 'ethereum',\n",
       " 'shot',\n",
       " 'digital',\n",
       " 'cash',\n",
       " 'surged',\n",
       " 'bitshares',\n",
       " 'jumped',\n",
       " 'money',\n",
       " 'might',\n",
       " 'made',\n",
       " 'cryptocurrency',\n",
       " 'market',\n",
       " 'grows',\n",
       " 'estimated',\n",
       " 'billion',\n",
       " 'industry',\n",
       " 'much',\n",
       " 'lucrative',\n",
       " 'place',\n",
       " 'make',\n",
       " 'money',\n",
       " 'industry',\n",
       " 'set',\n",
       " 'explode',\n",
       " 'full',\n",
       " 'surging',\n",
       " 'billion',\n",
       " 'trillion',\n",
       " 'next',\n",
       " 'four',\n",
       " 'years',\n",
       " 'one',\n",
       " 'former',\n",
       " 'hedge',\n",
       " 'fund',\n",
       " 'manager',\n",
       " 'paul',\n",
       " 'mampilly',\n",
       " 'recommending',\n",
       " 'single',\n",
       " 'cryptocurrency',\n",
       " 'readers',\n",
       " 'mampilly',\n",
       " 'cryptocurrencies',\n",
       " 'massive',\n",
       " 'bubble',\n",
       " 'risky',\n",
       " 'mampilly',\n",
       " 'says',\n",
       " 'new',\n",
       " 'emerging',\n",
       " 'industry',\n",
       " 'sky',\n",
       " 'rocket',\n",
       " 'investment',\n",
       " 'history',\n",
       " 'bitcoin',\n",
       " 'marijuana',\n",
       " 'biotech',\n",
       " 'one',\n",
       " 'heed',\n",
       " 'mampilly',\n",
       " 'insight',\n",
       " 'made',\n",
       " 'return',\n",
       " 'crash',\n",
       " 'part',\n",
       " 'reason',\n",
       " 'barron',\n",
       " 'named',\n",
       " 'hedge',\n",
       " 'fund',\n",
       " 'one',\n",
       " 'world',\n",
       " 'mampilly',\n",
       " 'became',\n",
       " 'famous',\n",
       " 'helping',\n",
       " 'millionaires',\n",
       " 'make',\n",
       " 'millions',\n",
       " 'turned',\n",
       " 'legendary',\n",
       " 'walked',\n",
       " 'away',\n",
       " 'wall',\n",
       " 'street',\n",
       " 'help',\n",
       " 'main',\n",
       " 'street',\n",
       " 'americans',\n",
       " 'make',\n",
       " 'type',\n",
       " 'returns',\n",
       " 'love',\n",
       " 'people',\n",
       " 'already',\n",
       " 'flocked',\n",
       " 'get',\n",
       " 'insight',\n",
       " 'helped',\n",
       " 'rack',\n",
       " 'much',\n",
       " 'total',\n",
       " 'winning',\n",
       " 'gains',\n",
       " 'one',\n",
       " 'guy',\n",
       " 'retired',\n",
       " 'man',\n",
       " 'buffalo',\n",
       " 'wrote',\n",
       " 'made',\n",
       " 'thanks',\n",
       " 'paul',\n",
       " 'timely',\n",
       " 'advice',\n",
       " 'even',\n",
       " 'sent',\n",
       " 'copy',\n",
       " 'brokerage',\n",
       " 'statement',\n",
       " 'prove',\n",
       " 'could',\n",
       " 'pale',\n",
       " 'comparison',\n",
       " 'industry',\n",
       " 'set',\n",
       " 'explode',\n",
       " 'mampilly',\n",
       " 'recently',\n",
       " 'posted',\n",
       " 'new',\n",
       " 'video',\n",
       " 'website',\n",
       " 'giving',\n",
       " 'details',\n",
       " 'behind',\n",
       " 'surging',\n",
       " 'industry',\n",
       " 'best',\n",
       " 'reveals',\n",
       " 'one',\n",
       " 'company',\n",
       " 'need',\n",
       " 'buy',\n",
       " 'profit',\n",
       " 'biggest',\n",
       " 'boldest',\n",
       " 'investment',\n",
       " 'prediction',\n",
       " 'paul',\n",
       " 'decorated',\n",
       " 'career',\n",
       " 'type',\n",
       " 'recommendation',\n",
       " 'paul',\n",
       " 'used',\n",
       " 'give',\n",
       " 'billionaire',\n",
       " 'clients',\n",
       " 'today',\n",
       " 'paul',\n",
       " 'giving',\n",
       " 'main',\n",
       " 'street',\n",
       " 'americans',\n",
       " 'get',\n",
       " 'ground',\n",
       " 'floor',\n",
       " 'wednesday']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_words_lower_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "list_of_simple_documents = [\n",
    "\"\"\"\n",
    "I really love this film. Everyone talks about the GCI in Aquaman,but what moves me most is its style ,so called \"old fashioned fun\" .Here I'll talk about this.(This review is translated from Chinese by google translater.And I'm the writer of it.)\n",
    "\n",
    "First of all, the story of Aquaman is classic enough (that is, the old-fashioned set of some people), which is the journey of superhero treasure hunt. In an interview with James wan in\"WorldScreen\"magazine, James said that this is a very classic narrative technique. The growth of Arthur is like the growth of heroes in Spielberg, Cameron and Lucas movies. When Wan was a child, he watched the works of these masters grow up and always wanted to make such a movie, so this retro style is intentional. The facts also prove that the classic story will not be outdated. However, we can also understand from another angle that Wan did not try to try the dark and deep story style, but from the direction of being good at commercial blockbusters. In short, even if you don't have interest in the retro story, the level of the plot is definitely qualified in the super-Hero movie.\n",
    "\n",
    "Second, it is the retro feel of the picture. On the one hand, many dazzling special effects brought us back to Avatar and the Lord of the Rings until the shock of the first Star Wars, and the imaginative and fascinating atmosphere of the underwater world is obviously a tribute to the classic fantasy sci-fi movies, like those of Star Wars. Just like classic fantasy operatic movie opens the window of imagination for us. On the other hand, James indicates that the film is expected to be like special effects movies in japan(like Ultraman and Godzilla in last century ). No wonder the friend who watched the movie with me shouted \"The clothes looks cheap\" when he first saw the soldiers of Atlantis. ..... I believe that it is not a problem to add more cool special effects to the soldiers, but James chose a very traditional approach , allowing the actors to fight in plastic armor and add to the film. A little bit of childlike beauty! Seeing this armor design is a little touched. We seem to be able to see james that realize the childhood special movie dreams,hiding behind the viewfinder and snickering.\n",
    "\n",
    "And......To be honest,I'm little worried about Aquaman2,which is said to have been planned by Warner bros because the first episode is too successful,but I don't believe James will direct it again(just like SAW and Fast&Furious,he did once and did the BEST)And I'm also worried about Birds of Prey because the director is too young ...looking DCU(or DCEU) movies is funny ,we must stand some boring films ,but sometimes DC gives us BIG surprise😘\n",
    "\"\"\",    \n",
    "\"\"\"I'm at a bit of a quandary about how to review \"Aquaman\" here. It's not good, that's fair to say but I've a strange feeling it's about as good as its ridiculous source material would allow it to be.\n",
    "\n",
    "Jason Momoa is Arthur Curry. Hilariously he's supposed to be 29 years old. He's also the son of Atlanna, fleeing queen of Atlantis (Nicole Kidman) and Tom, a lighthouse keeper, played (in unconvincing de-aging CGI by Temurea Morrison). His half-brother Orm (Patrick Wilson) threatens to unite the undersea kingdoms and wage war on the land dwellers, so Arthur returns home to challenge his brother for control of the undersea realms.\n",
    "\n",
    "So, whilst it's not really acting as such, Jason Momoa is always watchable and his charisma will carry you through quite a distance. The film also has a few decent action sequences along the way, particularly the underwater battle that concludes it and the opening fight scene on board a submarine. That's about it for the positives though.\n",
    "\n",
    "On dozens of occasions, \"Aquaman\" is laugh out loud hilariously awful. The plot is terrible, a cliché ridden McGuffin hunt, a painfully obvious twist towards the end and some awful foreshadowing plot points. It has, noticeably, terrible music choices throughout - a horrible score of pretend wonder. The rest of the performances apart from Momoa are dreadful, particularly Willem Defoe and Dolph Lundgren but nobody else comes off particularly well. It's the script though, that should have been drowned. From \"This was your grandfathers knife, he was a frogman in World War 2, then they turned their back on him\" onwards it's a terrible collection of recycled everything you've heard before in action movies for the last thirty years. It's painfully unaware how unfunny it is too.\n",
    "\n",
    "The CGI is patchy, happier with real creatures and vehicles than it is with fictional characters. It often ignores its own internal logic too, at one point Curry hangs onto to a building to avoid a ten foot drop - but before and after that he happily falls hundreds of feet and shrugs it off. Conventional weapons can't damage Curry, until towards the end, where one can. Nobody runs away when a shark starts to crack an aquarium wall.\n",
    "\n",
    "Sadly, I'm not convinced there is a better \"Aquaman\" movie possible than this one but I'm not sure that's any reason to pretend this one is any good.\n",
    "\"\"\",    \n",
    "\"\"\"Tarantino is without a doubt one of the best directors of all time and maybe the best of the 90's. His first film, Reservoir Dogs was amazing and claustrophobic, his segment in Four Rooms was by far the greatest (even though Rodriguez's was excellent too)and Jackie Brown is a wonderful homage to the Blaxploitation films of the 70's. However, Pulp Fiction remains my favourite.\n",
    "\n",
    "It was nominated for so many Oscars that I still find it hard to believe that it only got one: Best original script. I'm not complaining because Forrest Gump got best picture, since that film was also Oscar-worthy, but come on, movies like Tarantino's or the Shawshank Redemption deserved much more.\n",
    "\n",
    "Anyway, going back to the movie, I particularly liked the first and second chapters, and that's really a contradiction because one of the movie's finest characters, Mr. Wolf, appears on the third. Bruce Willis also does a great job, and as far as I'm concerned he fell in love with the movie right after having read the script. I like the way his character gives a \"tough guy\" image at the beginning and then we discover he's so affectionate and tender to his wife. Travolta is obviously the star of the movie and his second encounter with Bruce Willis in the kitchen along with the scene where he dances with Uma Thurman is when the movie reaches it's highest point.\n",
    "\n",
    "The other star is Samuel L. Jackson, who plays a wise assassin that obviously knows how to handle situations. \"And I will strike down upon thee with great vengeance and furious anger...\" is my favourite quote.\n",
    "\n",
    "Summarizing, Pulp Fiction is a modern classic and a must-see for anyone who is at least aware of what a movie is. I give it a 9 out of 10.\n",
    "\"\"\",    \n",
    "\"\"\"The proof of the pudding, they say, is in the eating. The proof of the movie is in the watching. Most of the top 250 IMDb movies have kept me glued to my seat--with this one I found my mind wandering to that jigsaw puzzle I hadn't finished or the possibility of some popcorn. I found I had very little interest in the characters or in what was going on.\n",
    "\n",
    "I asked myself why. Technically the film is very good. The actors all hit their marks and Samuel L. Jackson is particularly outstanding. I liked Maria de Madeiros also as Bruce Willis' wife Fabienne. The camera work is occasionally interesting, as the long scene where we watch Bruce Willis listen unemotionally to Marsellus go on. Interesting, certainly, but rather pointless.\n",
    "\n",
    "Indeed, that's the problem: so much of what goes on is pointless. It's a big long shaggy dog story, told by one of those irritating people who can't get the story straight and have to keep going back: \"Oh yeah, I forgot to tell you about that. Well, you know what I was telling you before . . .\" I tried to find some justification for the higgledy-piggledy way in which this story is told. It does result in the best scene being the last one. But if this scene was the point then why not design the script so that the action is seen to be moving toward this goal and cut out everything that happens afterward? In the end I don't think Tarentino knew what story he was telling and that's why so much is so pointless.\n",
    "\n",
    "The scenes of Butch attempting to control his temper, of his dilemma whether to help Marsellus, and the final scene in the restaurant are all good and entertaining as far as they go but they don't fall into a coherent framework. And the rest is quite dull.\n",
    "\n",
    "The dialogue is not witty or clever although it occasionally has its moments. The constant profanity is as pointless as the rest; the point of profanity is presumably to emphasize what one is saying, but if everything is emphasized, nothing is. The mind becomes numbed by it. It's like someone who shouts all the time. Eventually you stop listening. The quotes give you a pretty good idea of what the dialogue is like: when \"Shut the f*ck up, fat man!\" is listed as a memorable quote, you know how inane the conversation is.\n",
    "\n",
    "That this poorly composed script should have won an Oscar is a pretty clear indictment of the Academy.\n",
    "\"\"\",    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "470\n",
      "4\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text_words_lower = []\n",
    "\n",
    "for doc in list_of_simple_documents:\n",
    "    \n",
    "    text_words_lower.append([w for w in word_tokenize(doc.lower()) if w.isalpha()])\n",
    "\n",
    "print(len(text_words_lower))\n",
    "print(len(text_words_lower[0]))\n",
    "\n",
    "\n",
    "text_words_lower_nostop = []\n",
    "\n",
    "for doc in text_words_lower:\n",
    "    \n",
    "    text_words_lower_nostop.append([w for w in doc if w not in stopwords.words('english')])\n",
    "    \n",
    "    \n",
    "print(len(text_words_lower_nostop))\n",
    "print(len(text_words_lower_nostop[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'able': 0,\n",
       " 'actors': 1,\n",
       " 'add': 2,\n",
       " 'allowing': 3,\n",
       " 'also': 4,\n",
       " 'always': 5,\n",
       " 'angle': 6,\n",
       " 'another': 7,\n",
       " 'approach': 8,\n",
       " 'aquaman': 9,\n",
       " 'armor': 10,\n",
       " 'arthur': 11,\n",
       " 'atlantis': 12,\n",
       " 'atmosphere': 13,\n",
       " 'avatar': 14,\n",
       " 'back': 15,\n",
       " 'beauty': 16,\n",
       " 'behind': 17,\n",
       " 'believe': 18,\n",
       " 'best': 19,\n",
       " 'big': 20,\n",
       " 'birds': 21,\n",
       " 'bit': 22,\n",
       " 'blockbusters': 23,\n",
       " 'boring': 24,\n",
       " 'bros': 25,\n",
       " 'brought': 26,\n",
       " 'called': 27,\n",
       " 'cameron': 28,\n",
       " 'century': 29,\n",
       " 'cheap': 30,\n",
       " 'child': 31,\n",
       " 'childhood': 32,\n",
       " 'childlike': 33,\n",
       " 'chinese': 34,\n",
       " 'chose': 35,\n",
       " 'classic': 36,\n",
       " 'clothes': 37,\n",
       " 'commercial': 38,\n",
       " 'cool': 39,\n",
       " 'dark': 40,\n",
       " 'dazzling': 41,\n",
       " 'dc': 42,\n",
       " 'dceu': 43,\n",
       " 'dcu': 44,\n",
       " 'deep': 45,\n",
       " 'definitely': 46,\n",
       " 'design': 47,\n",
       " 'direct': 48,\n",
       " 'direction': 49,\n",
       " 'director': 50,\n",
       " 'dreams': 51,\n",
       " 'effects': 52,\n",
       " 'enough': 53,\n",
       " 'episode': 54,\n",
       " 'even': 55,\n",
       " 'everyone': 56,\n",
       " 'expected': 57,\n",
       " 'facts': 58,\n",
       " 'fantasy': 59,\n",
       " 'fascinating': 60,\n",
       " 'fashioned': 61,\n",
       " 'fast': 62,\n",
       " 'feel': 63,\n",
       " 'fight': 64,\n",
       " 'film': 65,\n",
       " 'films': 66,\n",
       " 'first': 67,\n",
       " 'friend': 68,\n",
       " 'fun': 69,\n",
       " 'funny': 70,\n",
       " 'furious': 71,\n",
       " 'gci': 72,\n",
       " 'gives': 73,\n",
       " 'godzilla': 74,\n",
       " 'good': 75,\n",
       " 'google': 76,\n",
       " 'grow': 77,\n",
       " 'growth': 78,\n",
       " 'hand': 79,\n",
       " 'heroes': 80,\n",
       " 'hiding': 81,\n",
       " 'honest': 82,\n",
       " 'however': 83,\n",
       " 'hunt': 84,\n",
       " 'imagination': 85,\n",
       " 'imaginative': 86,\n",
       " 'indicates': 87,\n",
       " 'intentional': 88,\n",
       " 'interest': 89,\n",
       " 'interview': 90,\n",
       " 'james': 91,\n",
       " 'japan': 92,\n",
       " 'journey': 93,\n",
       " 'last': 94,\n",
       " 'level': 95,\n",
       " 'like': 96,\n",
       " 'little': 97,\n",
       " 'looking': 98,\n",
       " 'looks': 99,\n",
       " 'lord': 100,\n",
       " 'love': 101,\n",
       " 'lucas': 102,\n",
       " 'magazine': 103,\n",
       " 'make': 104,\n",
       " 'many': 105,\n",
       " 'masters': 106,\n",
       " 'moves': 107,\n",
       " 'movie': 108,\n",
       " 'movies': 109,\n",
       " 'must': 110,\n",
       " 'narrative': 111,\n",
       " 'obviously': 112,\n",
       " 'old': 113,\n",
       " 'one': 114,\n",
       " 'opens': 115,\n",
       " 'operatic': 116,\n",
       " 'outdated': 117,\n",
       " 'people': 118,\n",
       " 'picture': 119,\n",
       " 'planned': 120,\n",
       " 'plastic': 121,\n",
       " 'plot': 122,\n",
       " 'prey': 123,\n",
       " 'problem': 124,\n",
       " 'prove': 125,\n",
       " 'qualified': 126,\n",
       " 'realize': 127,\n",
       " 'really': 128,\n",
       " 'retro': 129,\n",
       " 'review': 130,\n",
       " 'rings': 131,\n",
       " 'said': 132,\n",
       " 'saw': 133,\n",
       " 'second': 134,\n",
       " 'see': 135,\n",
       " 'seeing': 136,\n",
       " 'seem': 137,\n",
       " 'set': 138,\n",
       " 'shock': 139,\n",
       " 'short': 140,\n",
       " 'shouted': 141,\n",
       " 'snickering': 142,\n",
       " 'soldiers': 143,\n",
       " 'sometimes': 144,\n",
       " 'special': 145,\n",
       " 'spielberg': 146,\n",
       " 'stand': 147,\n",
       " 'star': 148,\n",
       " 'story': 149,\n",
       " 'style': 150,\n",
       " 'successful': 151,\n",
       " 'superhero': 152,\n",
       " 'talk': 153,\n",
       " 'talks': 154,\n",
       " 'technique': 155,\n",
       " 'touched': 156,\n",
       " 'traditional': 157,\n",
       " 'translated': 158,\n",
       " 'treasure': 159,\n",
       " 'tribute': 160,\n",
       " 'try': 161,\n",
       " 'ultraman': 162,\n",
       " 'understand': 163,\n",
       " 'underwater': 164,\n",
       " 'us': 165,\n",
       " 'viewfinder': 166,\n",
       " 'wan': 167,\n",
       " 'wanted': 168,\n",
       " 'warner': 169,\n",
       " 'wars': 170,\n",
       " 'watched': 171,\n",
       " 'window': 172,\n",
       " 'wonder': 173,\n",
       " 'works': 174,\n",
       " 'world': 175,\n",
       " 'worldscreen': 176,\n",
       " 'worried': 177,\n",
       " 'writer': 178,\n",
       " 'young': 179,\n",
       " 'acting': 180,\n",
       " 'action': 181,\n",
       " 'allow': 182,\n",
       " 'along': 183,\n",
       " 'apart': 184,\n",
       " 'aquarium': 185,\n",
       " 'atlanna': 186,\n",
       " 'avoid': 187,\n",
       " 'away': 188,\n",
       " 'awful': 189,\n",
       " 'battle': 190,\n",
       " 'better': 191,\n",
       " 'board': 192,\n",
       " 'brother': 193,\n",
       " 'building': 194,\n",
       " 'ca': 195,\n",
       " 'carry': 196,\n",
       " 'cgi': 197,\n",
       " 'challenge': 198,\n",
       " 'characters': 199,\n",
       " 'charisma': 200,\n",
       " 'choices': 201,\n",
       " 'cliché': 202,\n",
       " 'collection': 203,\n",
       " 'comes': 204,\n",
       " 'concludes': 205,\n",
       " 'control': 206,\n",
       " 'conventional': 207,\n",
       " 'convinced': 208,\n",
       " 'crack': 209,\n",
       " 'creatures': 210,\n",
       " 'curry': 211,\n",
       " 'damage': 212,\n",
       " 'decent': 213,\n",
       " 'defoe': 214,\n",
       " 'distance': 215,\n",
       " 'dolph': 216,\n",
       " 'dozens': 217,\n",
       " 'dreadful': 218,\n",
       " 'drop': 219,\n",
       " 'drowned': 220,\n",
       " 'dwellers': 221,\n",
       " 'else': 222,\n",
       " 'end': 223,\n",
       " 'everything': 224,\n",
       " 'fair': 225,\n",
       " 'falls': 226,\n",
       " 'feeling': 227,\n",
       " 'feet': 228,\n",
       " 'fictional': 229,\n",
       " 'fleeing': 230,\n",
       " 'foot': 231,\n",
       " 'foreshadowing': 232,\n",
       " 'frogman': 233,\n",
       " 'grandfathers': 234,\n",
       " 'hangs': 235,\n",
       " 'happier': 236,\n",
       " 'happily': 237,\n",
       " 'heard': 238,\n",
       " 'hilariously': 239,\n",
       " 'home': 240,\n",
       " 'horrible': 241,\n",
       " 'hundreds': 242,\n",
       " 'ignores': 243,\n",
       " 'internal': 244,\n",
       " 'jason': 245,\n",
       " 'keeper': 246,\n",
       " 'kidman': 247,\n",
       " 'kingdoms': 248,\n",
       " 'knife': 249,\n",
       " 'land': 250,\n",
       " 'laugh': 251,\n",
       " 'lighthouse': 252,\n",
       " 'logic': 253,\n",
       " 'loud': 254,\n",
       " 'lundgren': 255,\n",
       " 'material': 256,\n",
       " 'mcguffin': 257,\n",
       " 'momoa': 258,\n",
       " 'morrison': 259,\n",
       " 'music': 260,\n",
       " 'nicole': 261,\n",
       " 'nobody': 262,\n",
       " 'noticeably': 263,\n",
       " 'obvious': 264,\n",
       " 'occasions': 265,\n",
       " 'often': 266,\n",
       " 'onto': 267,\n",
       " 'onwards': 268,\n",
       " 'opening': 269,\n",
       " 'orm': 270,\n",
       " 'painfully': 271,\n",
       " 'particularly': 272,\n",
       " 'patchy': 273,\n",
       " 'patrick': 274,\n",
       " 'performances': 275,\n",
       " 'played': 276,\n",
       " 'point': 277,\n",
       " 'points': 278,\n",
       " 'positives': 279,\n",
       " 'possible': 280,\n",
       " 'pretend': 281,\n",
       " 'quandary': 282,\n",
       " 'queen': 283,\n",
       " 'quite': 284,\n",
       " 'real': 285,\n",
       " 'realms': 286,\n",
       " 'reason': 287,\n",
       " 'recycled': 288,\n",
       " 'rest': 289,\n",
       " 'returns': 290,\n",
       " 'ridden': 291,\n",
       " 'ridiculous': 292,\n",
       " 'runs': 293,\n",
       " 'sadly': 294,\n",
       " 'say': 295,\n",
       " 'scene': 296,\n",
       " 'score': 297,\n",
       " 'script': 298,\n",
       " 'sequences': 299,\n",
       " 'shark': 300,\n",
       " 'shrugs': 301,\n",
       " 'son': 302,\n",
       " 'source': 303,\n",
       " 'starts': 304,\n",
       " 'strange': 305,\n",
       " 'submarine': 306,\n",
       " 'supposed': 307,\n",
       " 'sure': 308,\n",
       " 'temurea': 309,\n",
       " 'ten': 310,\n",
       " 'terrible': 311,\n",
       " 'thirty': 312,\n",
       " 'though': 313,\n",
       " 'threatens': 314,\n",
       " 'throughout': 315,\n",
       " 'tom': 316,\n",
       " 'towards': 317,\n",
       " 'turned': 318,\n",
       " 'twist': 319,\n",
       " 'unaware': 320,\n",
       " 'unconvincing': 321,\n",
       " 'undersea': 322,\n",
       " 'unfunny': 323,\n",
       " 'unite': 324,\n",
       " 'vehicles': 325,\n",
       " 'wage': 326,\n",
       " 'wall': 327,\n",
       " 'war': 328,\n",
       " 'watchable': 329,\n",
       " 'way': 330,\n",
       " 'weapons': 331,\n",
       " 'well': 332,\n",
       " 'whilst': 333,\n",
       " 'willem': 334,\n",
       " 'wilson': 335,\n",
       " 'would': 336,\n",
       " 'years': 337,\n",
       " 'affectionate': 338,\n",
       " 'amazing': 339,\n",
       " 'anger': 340,\n",
       " 'anyone': 341,\n",
       " 'anyway': 342,\n",
       " 'appears': 343,\n",
       " 'assassin': 344,\n",
       " 'aware': 345,\n",
       " 'beginning': 346,\n",
       " 'blaxploitation': 347,\n",
       " 'brown': 348,\n",
       " 'bruce': 349,\n",
       " 'chapters': 350,\n",
       " 'character': 351,\n",
       " 'claustrophobic': 352,\n",
       " 'come': 353,\n",
       " 'complaining': 354,\n",
       " 'concerned': 355,\n",
       " 'contradiction': 356,\n",
       " 'dances': 357,\n",
       " 'deserved': 358,\n",
       " 'directors': 359,\n",
       " 'discover': 360,\n",
       " 'dogs': 361,\n",
       " 'doubt': 362,\n",
       " 'encounter': 363,\n",
       " 'excellent': 364,\n",
       " 'far': 365,\n",
       " 'favourite': 366,\n",
       " 'fell': 367,\n",
       " 'fiction': 368,\n",
       " 'find': 369,\n",
       " 'finest': 370,\n",
       " 'forrest': 371,\n",
       " 'four': 372,\n",
       " 'give': 373,\n",
       " 'going': 374,\n",
       " 'got': 375,\n",
       " 'great': 376,\n",
       " 'greatest': 377,\n",
       " 'gump': 378,\n",
       " 'guy': 379,\n",
       " 'handle': 380,\n",
       " 'hard': 381,\n",
       " 'highest': 382,\n",
       " 'homage': 383,\n",
       " 'image': 384,\n",
       " 'jackie': 385,\n",
       " 'jackson': 386,\n",
       " 'job': 387,\n",
       " 'kitchen': 388,\n",
       " 'knows': 389,\n",
       " 'least': 390,\n",
       " 'liked': 391,\n",
       " 'maybe': 392,\n",
       " 'modern': 393,\n",
       " 'much': 394,\n",
       " 'nominated': 395,\n",
       " 'original': 396,\n",
       " 'oscars': 397,\n",
       " 'plays': 398,\n",
       " 'pulp': 399,\n",
       " 'quote': 400,\n",
       " 'reaches': 401,\n",
       " 'read': 402,\n",
       " 'redemption': 403,\n",
       " 'remains': 404,\n",
       " 'reservoir': 405,\n",
       " 'right': 406,\n",
       " 'rodriguez': 407,\n",
       " 'rooms': 408,\n",
       " 'samuel': 409,\n",
       " 'segment': 410,\n",
       " 'shawshank': 411,\n",
       " 'since': 412,\n",
       " 'situations': 413,\n",
       " 'still': 414,\n",
       " 'strike': 415,\n",
       " 'summarizing': 416,\n",
       " 'tarantino': 417,\n",
       " 'tender': 418,\n",
       " 'thee': 419,\n",
       " 'third': 420,\n",
       " 'thurman': 421,\n",
       " 'time': 422,\n",
       " 'tough': 423,\n",
       " 'travolta': 424,\n",
       " 'uma': 425,\n",
       " 'upon': 426,\n",
       " 'vengeance': 427,\n",
       " 'wife': 428,\n",
       " 'willis': 429,\n",
       " 'wise': 430,\n",
       " 'without': 431,\n",
       " 'wolf': 432,\n",
       " 'wonderful': 433,\n",
       " 'academy': 434,\n",
       " 'afterward': 435,\n",
       " 'although': 436,\n",
       " 'asked': 437,\n",
       " 'attempting': 438,\n",
       " 'becomes': 439,\n",
       " 'butch': 440,\n",
       " 'camera': 441,\n",
       " 'certainly': 442,\n",
       " 'clear': 443,\n",
       " 'clever': 444,\n",
       " 'coherent': 445,\n",
       " 'composed': 446,\n",
       " 'constant': 447,\n",
       " 'conversation': 448,\n",
       " 'cut': 449,\n",
       " 'de': 450,\n",
       " 'dialogue': 451,\n",
       " 'dilemma': 452,\n",
       " 'dog': 453,\n",
       " 'dull': 454,\n",
       " 'eating': 455,\n",
       " 'emphasize': 456,\n",
       " 'emphasized': 457,\n",
       " 'entertaining': 458,\n",
       " 'eventually': 459,\n",
       " 'fabienne': 460,\n",
       " 'fall': 461,\n",
       " 'fat': 462,\n",
       " 'final': 463,\n",
       " 'finished': 464,\n",
       " 'forgot': 465,\n",
       " 'found': 466,\n",
       " 'framework': 467,\n",
       " 'get': 468,\n",
       " 'glued': 469,\n",
       " 'go': 470,\n",
       " 'goal': 471,\n",
       " 'goes': 472,\n",
       " 'happens': 473,\n",
       " 'help': 474,\n",
       " 'hit': 475,\n",
       " 'idea': 476,\n",
       " 'imdb': 477,\n",
       " 'inane': 478,\n",
       " 'indeed': 479,\n",
       " 'indictment': 480,\n",
       " 'interesting': 481,\n",
       " 'irritating': 482,\n",
       " 'jigsaw': 483,\n",
       " 'justification': 484,\n",
       " 'keep': 485,\n",
       " 'kept': 486,\n",
       " 'knew': 487,\n",
       " 'know': 488,\n",
       " 'listed': 489,\n",
       " 'listen': 490,\n",
       " 'listening': 491,\n",
       " 'long': 492,\n",
       " 'madeiros': 493,\n",
       " 'man': 494,\n",
       " 'maria': 495,\n",
       " 'marks': 496,\n",
       " 'marsellus': 497,\n",
       " 'memorable': 498,\n",
       " 'mind': 499,\n",
       " 'moments': 500,\n",
       " 'moving': 501,\n",
       " 'nothing': 502,\n",
       " 'numbed': 503,\n",
       " 'occasionally': 504,\n",
       " 'oh': 505,\n",
       " 'oscar': 506,\n",
       " 'outstanding': 507,\n",
       " 'pointless': 508,\n",
       " 'poorly': 509,\n",
       " 'popcorn': 510,\n",
       " 'possibility': 511,\n",
       " 'presumably': 512,\n",
       " 'pretty': 513,\n",
       " 'profanity': 514,\n",
       " 'proof': 515,\n",
       " 'pudding': 516,\n",
       " 'puzzle': 517,\n",
       " 'quotes': 518,\n",
       " 'rather': 519,\n",
       " 'restaurant': 520,\n",
       " 'result': 521,\n",
       " 'saying': 522,\n",
       " 'scenes': 523,\n",
       " 'seat': 524,\n",
       " 'seen': 525,\n",
       " 'shaggy': 526,\n",
       " 'shouts': 527,\n",
       " 'shut': 528,\n",
       " 'someone': 529,\n",
       " 'stop': 530,\n",
       " 'straight': 531,\n",
       " 'tarentino': 532,\n",
       " 'technically': 533,\n",
       " 'tell': 534,\n",
       " 'telling': 535,\n",
       " 'temper': 536,\n",
       " 'think': 537,\n",
       " 'told': 538,\n",
       " 'top': 539,\n",
       " 'toward': 540,\n",
       " 'tried': 541,\n",
       " 'unemotionally': 542,\n",
       " 'wandering': 543,\n",
       " 'watch': 544,\n",
       " 'watching': 545,\n",
       " 'whether': 546,\n",
       " 'witty': 547,\n",
       " 'work': 548,\n",
       " 'yeah': 549}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dictionary = Dictionary(text_words_lower_nostop)\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "4\n",
      "<class 'list'>\n",
      "<class 'tuple'>\n",
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 3), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 5), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 3), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 3), (66, 1), (67, 4), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 6), (92, 1), (93, 1), (94, 1), (95, 1), (96, 6), (97, 3), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 5), (109, 4), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 3), (130, 1), (131, 1), (132, 2), (133, 2), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 2), (144, 1), (145, 4), (146, 1), (147, 1), (148, 2), (149, 4), (150, 3), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 2), (162, 1), (163, 1), (164, 1), (165, 3), (166, 1), (167, 3), (168, 1), (169, 1), (170, 2), (171, 2), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 2), (178, 1), (179, 1)]\n",
      "----\n",
      "[(4, 2), (5, 1), (9, 3), (11, 2), (12, 1), (15, 1), (22, 1), (64, 1), (65, 1), (75, 3), (84, 1), (94, 1), (108, 1), (109, 1), (113, 1), (114, 4), (122, 2), (128, 1), (130, 1), (164, 1), (173, 1), (175, 1), (180, 1), (181, 2), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 2), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 3), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 2), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 2), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 2), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 3), (259, 1), (260, 1), (261, 1), (262, 2), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 2), (272, 3), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 2), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 3), (312, 1), (313, 2), (314, 1), (315, 1), (316, 1), (317, 2), (318, 1), (319, 1), (320, 1), (321, 1), (322, 2), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 2), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in text_words_lower_nostop]\n",
    "\n",
    "print(type(corpus))\n",
    "print(len(corpus))\n",
    "\n",
    "print(type(corpus[0]))\n",
    "print(type(corpus[0][0]))\n",
    "print(corpus[0])\n",
    "print(\"----\")\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 2), (15, 1), (18, 1), (19, 4), (36, 1), (55, 1), (65, 2), (66, 1), (67, 2), (71, 1)]\n"
     ]
    }
   ],
   "source": [
    "# print the first 10 word ids with their frequency counts of the 3rd doc\n",
    "\n",
    "print(corpus[2][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hitmap of shared words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "james 6\n",
      "like 6\n",
      "classic 5\n",
      "movie 5\n",
      "first 4\n",
      "movies 4\n",
      "special 4\n",
      "story 4\n",
      "also 3\n",
      "effects 3\n",
      "film 3\n",
      "little 3\n",
      "retro 3\n",
      "style 3\n",
      "us 3\n",
      "wan 3\n",
      "add 2\n",
      "aquaman 2\n",
      "armor 2\n",
      "believe 2\n",
      "------------\n",
      "one 4\n",
      "aquaman 3\n",
      "good 3\n",
      "curry 3\n",
      "momoa 3\n",
      "particularly 3\n",
      "terrible 3\n",
      "also 2\n",
      "arthur 2\n",
      "plot 2\n",
      "action 2\n",
      "awful 2\n",
      "cgi 2\n",
      "end 2\n",
      "hilariously 2\n",
      "jason 2\n",
      "nobody 2\n",
      "painfully 2\n",
      "pretend 2\n",
      "though 2\n",
      "------------\n",
      "movie 6\n",
      "best 4\n",
      "one 3\n",
      "also 2\n",
      "film 2\n",
      "first 2\n",
      "like 2\n",
      "obviously 2\n",
      "second 2\n",
      "star 2\n",
      "script 2\n",
      "bruce 2\n",
      "far 2\n",
      "favourite 2\n",
      "fiction 2\n",
      "got 2\n",
      "great 2\n",
      "pulp 2\n",
      "tarantino 2\n",
      "willis 2\n",
      "------------\n",
      "one 4\n",
      "story 4\n",
      "scene 4\n",
      "pointless 4\n",
      "good 3\n",
      "like 2\n",
      "everything 2\n",
      "point 2\n",
      "rest 2\n",
      "script 2\n",
      "bruce 2\n",
      "going 2\n",
      "much 2\n",
      "willis 2\n",
      "dialogue 2\n",
      "found 2\n",
      "go 2\n",
      "interesting 2\n",
      "know 2\n",
      "long 2\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,len(corpus)):\n",
    "\n",
    "    doc = corpus[i]\n",
    "\n",
    "    doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    for word_id, word_count in doc[:20]:\n",
    "\n",
    "        print(dictionary.get(word_id), word_count)\n",
    "\n",
    "    print(\"------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'somekey'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-2d82dcbd3dd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_reg_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_reg_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"somekey\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'somekey'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_reg_dict = {\"name\": \"john\", \"age\": 33}\n",
    "\n",
    "print(test_reg_dict[\"name\"])\n",
    "\n",
    "print(test_reg_dict[\"somekey\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# defaultdict\n",
    "\n",
    "import collections\n",
    "\n",
    "print(int())\n",
    "\n",
    "\n",
    "test_defaultdict = collections.defaultdict(int)\n",
    "\n",
    "print(test_defaultdict[\"somekey\"])\n",
    "\n",
    "#\n",
    "\n",
    "test_defaultdict = collections.defaultdict(list)\n",
    "\n",
    "print(test_defaultdict[\"somekey\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.chain object at 0x00000202D9BEAFD0>\n",
      "[1, 2, 3, 'a', 'b', 'c']\n",
      "[1, 2, 3, 'a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = ['a', 'b', 'c']\n",
    "\n",
    "print(itertools.chain(a,b))\n",
    "\n",
    "print(list(itertools.chain(a,b)))\n",
    "\n",
    "#\n",
    "\n",
    "c = [a, b]\n",
    "\n",
    "print(list(itertools.chain.from_iterable(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# += adds another value with the variable's value and assigns the new value to the variable.\n",
    "\n",
    "x = 2\n",
    "\n",
    "x += 3\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie 13\n",
      "one 12\n",
      "like 10\n",
      "also 8\n",
      "story 8\n",
      "film 7\n",
      "good 7\n",
      "movies 7\n",
      "best 6\n",
      "classic 6\n",
      "first 6\n",
      "james 6\n",
      "scene 6\n",
      "aquaman 5\n",
      "particularly 5\n",
      "script 5\n",
      "back 4\n",
      "little 4\n",
      "special 4\n",
      "star 4\n"
     ]
    }
   ],
   "source": [
    "# now we will print the top 20 words across all documents alongside the count\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "\n",
    "total_word_count = collections.defaultdict(int)\n",
    "\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    \n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# create a sorted list from the defaultdict using words across the entire corpus\n",
    "# to achieve this, use the .items() method on total_word_count inside sorted()\n",
    "\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# print the top 20 words across all documents and their counts\n",
    "\n",
    "for word_id, word_count in sorted_word_count[:20]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with gensim\n",
    "\n",
    "### EXPLAIN FROM THE WORD OF COURSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.tfidfmodel.TfidfModel'>\n",
      "<class 'list'>\n",
      "[(0, 0.058902809974266904), (1, 0.029451404987133452), (2, 0.11780561994853381), (3, 0.058902809974266904), (5, 0.029451404987133452)]\n",
      "james 0.3534168598456014\n",
      "special 0.23561123989706761\n",
      "effects 0.1767084299228007\n",
      "retro 0.1767084299228007\n",
      "style 0.1767084299228007\n",
      "us 0.1767084299228007\n",
      "wan 0.1767084299228007\n",
      "classic 0.14725702493566725\n",
      "add 0.11780561994853381\n",
      "armor 0.11780561994853381\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# create a new TfidfModel using the corpus\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "print(type(tfidf))\n",
    "\n",
    "\n",
    "# calculate the tfidf weights for a specific doc\n",
    "\n",
    "tfidf_weights_doc0 = tfidf[corpus[0]]\n",
    "\n",
    "print(type(tfidf_weights_doc0))\n",
    "\n",
    "\n",
    "# print the first five weights - they are unsorted\n",
    "\n",
    "print(tfidf_weights_doc0[:5])\n",
    "\n",
    "\n",
    "# sort the weights from highest to lowest\n",
    "\n",
    "tfidf_weights_doc0_sorted = sorted(tfidf_weights_doc0, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# print top 10 weighted words\n",
    "\n",
    "for term_id, weight in tfidf_weights_doc0_sorted[:10]:\n",
    "    print(dictionary.get(term_id), weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "james 0.3534168598456014\n",
      "special 0.23561123989706761\n",
      "effects 0.1767084299228007\n",
      "retro 0.1767084299228007\n",
      "style 0.1767084299228007\n",
      "us 0.1767084299228007\n",
      "wan 0.1767084299228007\n",
      "classic 0.14725702493566725\n",
      "add 0.11780561994853381\n",
      "armor 0.11780561994853381\n",
      "-------------------\n",
      "curry 0.2066134318677899\n",
      "momoa 0.2066134318677899\n",
      "terrible 0.2066134318677899\n",
      "awful 0.13774228791185994\n",
      "cgi 0.13774228791185994\n",
      "hilariously 0.13774228791185994\n",
      "jason 0.13774228791185994\n",
      "nobody 0.13774228791185994\n",
      "painfully 0.13774228791185994\n",
      "pretend 0.13774228791185994\n",
      "-------------------\n",
      "favourite 0.1866699168051429\n",
      "fiction 0.1866699168051429\n",
      "got 0.1866699168051429\n",
      "great 0.1866699168051429\n",
      "pulp 0.1866699168051429\n",
      "tarantino 0.1866699168051429\n",
      "first 0.09333495840257144\n",
      "obviously 0.09333495840257144\n",
      "second 0.09333495840257144\n",
      "star 0.09333495840257144\n",
      "-------------------\n",
      "pointless 0.2897647904082805\n",
      "story 0.14488239520414026\n",
      "dialogue 0.14488239520414026\n",
      "found 0.14488239520414026\n",
      "go 0.14488239520414026\n",
      "interesting 0.14488239520414026\n",
      "know 0.14488239520414026\n",
      "long 0.14488239520414026\n",
      "marsellus 0.14488239520414026\n",
      "mind 0.14488239520414026\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# do the above for all docs\n",
    "\n",
    "# calculate the tfidf weights for a specific doc\n",
    "\n",
    "for i in range(0,len(corpus)):\n",
    "\n",
    "    tfidf_weights_doc = tfidf[corpus[i]]\n",
    "\n",
    "    # sort the weights from highest to lowest\n",
    "\n",
    "    tfidf_weights_doc_sorted = sorted(tfidf_weights_doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # print top 10 weighted words\n",
    "\n",
    "    for term_id, weight in tfidf_weights_doc_sorted[:10]:\n",
    "        print(dictionary.get(term_id), weight)\n",
    "\n",
    "    print(\"-------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "## explain and add to the word too\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_text = \"\"\"\n",
    "Apple Inc., formerly Apple Computer, Inc., is a multinational corporation that creates consumer electronics, personal computers, servers, and computer software, and is a digital distributor of media content. The company also has a chain of retail stores known as Apple Stores. Apple's core product lines are the iPhone smartphone, iPad tablet computer, iPodportable media players, and Macintosh computer line. Founders Steve Jobs and Steve Wozniak created Apple Computer on April 1, 1976,[1] and incorporated the company on January 3, 1977,[2] in Cupertino, California.\n",
    "For more than three decades, Apple Computer was predominantly a manufacturer of personal computers, including the Apple II, Macintosh, and Power Mac lines, but it faced rocky sales and low market share during the 1990s. Jobs, who had been ousted from the company in 1985, returned to Apple in 1996 after his company NeXT was bought by Apple.[3] The following year he became the company's interim CEO,[4] which later became permanent.[5] Jobs subsequently instilled a new corporate philosophy of recognizable products and simple design, starting with the original iMac in 1998.\n",
    "With the introduction of the successful iPod music player in 2001 and iTunes Music Storein 2003, Apple established itself as a leader in the consumer electronics and media sales industries, leading it to drop \"Computer\" from the company's name in 2007. The company is now also known for its iOS range of smart phone, media player, and tablet computer products that began with the iPhone, followed by the iPod Touch and then iPad. As of 30 June 2015, Apple was the largest publicly traded corporation in the world by market capitalization,[6] with an estimated value of US$1 trillion as of August 2, 2018.[7] Apple's worldwide annual revenue in 2010 totaled US$65 billion, growing to US$127.8 billion in 2011[8] and $156 billion in 2012.[9]\n",
    "Steve Jobs and Steve Wozniak had withdrawn from Reed College and UC Berkeley respectively by 1975. Wozniak designed a video terminal that he could use to log on to the minicomputers at Call Computer. Alex Kamradt commissioned the design and sold a small number of them through his firm. Aside from their interest in up-to-date technology, the impetus for Jobs and Wozniak, also referred to collectively as \"the two Steves\", seems to have had another source. In his essay From Satori to Silicon Valley (published 1986), cultural historian Theodore Roszak made the point that the Apple Computer emerged from within the West Coast countercultureand the need to produce print-outs, letter labels, and databases. Roszak offers a bit of background on the development of the two Steves' prototype models.\n",
    "In 1975, Wozniak started attending meetings of the Homebrew Computer Club. New microcomputers such as the Altair 8800 and the IMSAI inspired him to build a microprocessor into his video terminal and have a complete computer.\n",
    "At the time the only microcomputer CPUs generally available were the $179 Intel 8080(equivalent to $815 in 2017), and the $170 Motorola 6800 (equivalent to $774 in 2017). Wozniak preferred the 6800, but both were out of his price range. So he watched, and learned, and designed computers on paper, waiting for the day he could afford a CPU.\n",
    "When MOS Technology released its $20 (equivalent to $86 in 2017) 6502 chip in 1976, Wozniak wrote a version of BASIC for it, then began to design a computer for it to run on. The 6502 was designed by the same people who designed the 6800, as many in Silicon Valley left employers to form their own companies. Wozniak's earlier 6800 paper-computer needed only minor changes to run on the new chip.\n",
    "Wozniak completed the machine and took it to Homebrew Computer Club meetings to show it off. At the meeting, Wozniak met his old friend Jobs, who was interested in the commercial potential of the small hobby machines.\n",
    "The two Steves had been friends for some time, having met in 1971, when their mutual friend, Bill Fernandez, introduced then 21-year-old Wozniak to 16-year-old Jobs. They began their partnership when Wozniak, a talented, self-educated electronics engineer, began constructing boxes which enabled one to make long-distance phone calls at no cost, and sold several hundred models.[11] Later, Jobs managed to interest Wozniak in assembling a computer machine and selling it.\n",
    "Jobs approached a local computer store, The Byte Shop, who said they would be interested in the machine, but only if it came fully assembled. The owner, Paul Terrell, went further, saying he would order 50 of the machines and pay US $500 each on delivery (equivalent to $2,200 in 2017).[12]Jobs then took the purchase order that he had been given from the Byte Shop to Cramer Electronics, a national electronic parts distributor, and ordered the components he needed to assemble the Apple I Computer. The local credit manager asked Jobs how he was going to pay for the parts and he replied, \"I have this purchase order from the Byte Shop chain of computer stores for 50 of my computers and the payment terms are COD. If you give me the parts on a net 30-day terms I can build and deliver the computers in that time frame, collect my money from Terrell at the Byte Shop and pay you.\"[13][14]\n",
    "The credit manager called Paul Terrell, who was attending an IEEE computer conference at Asilomar in Pacific Grove, and verified the validity of the purchase order. Amazed at the tenacity of Jobs, Terrell assured the credit manager if the computers showed up in his stores, Jobs would be paid and would have more than enough money to pay for the parts order. The two Steves and their small crew spent day and night building and testing the computers, and delivered to Terrell on time to pay his suppliers and have a tidy profit left over for their celebration and next order. Steve Jobs had found a way to finance his soon-to-be multimillion-dollar company without giving away one share of stock or ownership.\n",
    "The machine had only a few notable features. One was the use of a TV as the display system, whereas many machines had no display at all. This was not like the displays of later machines, however; text was displayed at 60 characters per second. However, this was still faster than the teleprinters used on contemporary machines of that era. The Apple I also included bootstrap code on ROM, which made it easier to start up. Finally, at the insistence of Paul Terrell, Wozniak also designed a cassette interface for loading and saving programs, at the then-rapid pace of 1200 bit/s. Although the machine was fairly simple, it was nevertheless a masterpiece of design, using far fewer parts than anything in its class, and quickly earning Wozniak a reputation as a master designer.\n",
    "Joined by another friend, Ronald Wayne, the two Steves started building the machines. Using a variety of methods, including borrowing space from friends and family, selling various prized items (like Wozniak's HP scientific calculator and Jobs' VW bus) and scrounging, Jobs managed to secure the parts needed while Wozniak and Wayne assembled them. Building such a machine was going to be financially burdensome, and the owner of the Byte Shop was expecting complete computers, not just printed circuit boards. The boards being a product for the customers, Terrell still paid them.[15] Jobs started looking for cash, but banks were reluctant to lend him money; the idea of a computer for ordinary people seemed absurd at the time. Jobs eventually met Mike Markkula who co-signed a bank loan for $250,000 (equivalent to $1,080,000 in 2017), and Jobs, Wozniak and Wayne formed Apple Computer on April 1, 1976. Wayne was somewhat gun-shy due to a failed venture four years earlier and soon dropped out of the company, leaving the two Steves as the active primary co-founders.[1] The name Apple was chosen because the company to beat in the technology industry at the time was Atari, and Apple Computer came before Atari alphabetically and thus also in the phone book. Another reason was that Jobs had happy memories of working on an Oregon apple farm one summer.[16] Eventually, 200 of the Apple I's were built.\n",
    "Wozniak had soon moved on from the Apple I. Many of the design features of the I were due to the limited amount of money they had to construct the prototype, but with the income from the sales Wozniak was able to start construction of a greatly improved machine, the Apple II; the two Steves presented it to the public at the first West Coast Computer Faire on April 16 and 17, 1977. On the first day of the exhibition, Jobs introduced the Apple II to a Japanese chemist named Toshio Mizushima, who became the first authorized Apple dealer in Japan.\n",
    "The main difference internally was a completely redesigned TV interface, which held the display in memory. Now not only useful for simple text display, the Apple II included graphics and, eventually, color. Jobs meanwhile pressed for a much improved case and keyboard, with the idea that the machine should be complete and ready to run out of the box. This was almost the case for the Apple I machines sold to The Byte Shop, but one still needed to plug various parts together and type in the code to run BASIC.\n",
    "With both cash and a new case design in hand thanks to designer Jerry Manock, the Apple II was released in 1977 and was one of the three \"1977 Trinity\" computers generally credited with creating the home computer market (the other two being the Commodore PET and the Tandy Corporation TRS-80).[17] Millions were sold well into the 1980s. A number of different models of the Apple II series were built, including the Apple IIe and Apple IIGS, which continued in public use for nearly two decades thereafter.\n",
    "While the Apple II was already established as a successful business-ready platform because of Visicalc, Apple was not content. The Apple III was designed to take on the business environment. The Apple III was released on May 19, 1980.\n",
    "The Apple III was a relatively conservative design for computers of the era. However, Steve Jobs did not want the computer to have a fan; rather, he wanted the heat generated by the electronics to be dissipated through the chassis of the machine, forgoing the cooling fan.\n",
    "However, the physical design of the case was not sufficient to cool the components inside it. By removing the fan from the design, the Apple III was prone to overheating. This caused the integrated circuit chips to disconnect from the motherboard. Customers who contacted Apple customer service were told to \"raise the computers six inches in the air, and then let go\", which would cause the ICs to fall back into place.\n",
    "Thousands of Apple III computers were recalled. A new model was introduced in 1983 to try and rectify the problems, but the damage was already done.\n",
    "In the July 1980 issue of Kilobaud Microcomputing, publisher Wayne Green stated that \"the best consumer ads I've seen have been those by Apple. They are attention-getting, and they must be prompting sale.\"[18] In August, the Financial Times reported that\n",
    "Apple Computer, the fast growing Californian manufacturer of small computers for the consumer, business and educational markets, is planning to go public later this year. [It] is the largest private manufacturer in the U.S. of small computers. Founded about five years ago as a small workshop business, it has become the second largest manufacturer of small computers, after the Radio Shack division of the Tandy company.[19]\n",
    "On December 12, 1980, Apple launched the Initial Public Offering of its stock to the investing public. When Apple went public, it generated more capital than any IPO since Ford Motor Company in 1956 and instantly created more millionaires (about 300) than any company in history.[20] Several venture capitalists cashed out, reaping billions in long-term capital gains.\n",
    "In January 1981, Apple held its first shareholders meeting as a public company in the Flint Center, a large auditorium at nearby De Anza College (which is often used for symphony concerts) to handle the larger numbers of shareholders post-IPO. The business of the meeting had been planned so that the voting could be staged in 15 minutes or less. In most cases, voting proxies are collected by mail and counted days or months before a meeting. In this case, after the IPO, many shares were in new hands.\n",
    "Steve Jobs started his prepared speech, but after being interrupted by voting several times, he dropped his prepared speech and delivered a long, emotionally charged talk about betrayal, lack of respect, and related topics.[citation needed]\n",
    "By August 1981 Apple was among the three largest microcomputer companies, perhaps having replaced Radio Shack as the leader.[21] IBM entered the personal computer market that month with the IBM PC,[22] but Apple had many advantages. While IBM began with one microcomputer, little available hardware or software, and a couple of hundred dealers, Apple had five times as many dealers in the US and an established international distribution network. The Apple II had an installed base of more than 250,000 customers, and hundreds of independent developers offered software and peripherals; at least ten databases and ten word processors were available, while the PC had no databases and one word processor.[23] The company's customers gained a reputation for devotion and loyalty. BYTE in 1984 stated that[24]\n",
    "There are two kinds of people in the world: people who say Apple isn't just a company, it's a cause; and people who say Apple isn't a cause, it's just a company. Both groups are right. Nature has suspended the principle of noncontradiction where Apple is concerned.\n",
    "Apple is more than just a company because its founding has some of the qualities of myth ... Apple is two guys in a garage undertaking the mission of bringing computing power, once reserved for big corporations, to ordinary individuals with ordinary budgets. The company's growth from two guys to a billion-dollar corporation exemplifies the American Dream. Even as a large corporation, Apple plays David to IBM's Goliath, and thus has the sympathetic role in that myth.\n",
    "The magazine noted, however, that the loyalty was not entirely positive for Apple; customers were willing to overlook real flaws in its products, even while comparing the company to a higher standard than for competitors.[24] The Apple III was an example of the company's reputation among dealers that one described as \"Apple arrogance\".[25][26]After examining a PC and finding it unimpressive, Apple confidently purchased a full-page advertisement in The Wall Street Journal with the headline \"Welcome, IBM. Seriously\". Microsoft head Bill Gates was at Apple headquarters the day of IBM's announcement and later said \"They didn't seem to care. It took them a full year to realize what had happened\".[22] By 1983 the PC surpassed the Apple II as the best-selling personal computer.[27] By 1984 IBM had $4 billion in annual PC revenue, more than twice that of Apple and as much as the sales of it and the next three companies combined.[28] Most Apple II sales had been to companies,[29] but a Fortune survey found that 56% of American companies with personal computers used IBM PCs, compared to 16% for Apple.[30] Small businesses, schools, and some homes became the II's primary market.[29]\n",
    "Apple Computer's business division was focused on the Apple III, another iteration of the text-based computer. Simultaneously the Lisa group worked on a new machine that would feature a completely different interface and introduce the words mouse, icon, and desktop into the lexiconof the computing public. In return for the right to buy US$1,000,000 of pre-IPO stock, Xerox granted Apple Computer three days access to the PARC facilities. After visiting PARC, they came away with new ideas that would complete the foundation for Apple Computer's first GUIcomputer, the Apple Lisa.[31][32][33][34]\n",
    "The first iteration of Apple's WIMP interface was a floppy disk where files could be spatially moved around. After months of usability testing, Apple designed the Lisa interface of windows and icons.\n",
    "The Lisa was introduced in 1983 at a cost of US $9,995 (equivalent to $24,600 in 2017). Because of the high price, Lisa failed to penetrate the business market.\n",
    "The Macintosh 128k was announced to the press in October 1983, followed by an 18-page brochure included with various magazines in December.[35] Its debut, however, was announced by a single national broadcast of the now famous US$1.5 million television commercial, \"1984\" (equivalent to $3,500,000 in 2017). It was directed by Ridley Scott, aired during the third quarter of Super Bowl XVIII on January 22, 1984,[36] and is now considered a \"watershed event\"[37] and a \"masterpiece.\"[38] 1984 used an unnamed heroine to represent the coming of the Macintosh (indicated by her white tank top with a Picasso-style picture of Apple's Macintosh computer on it) as a means of saving humanity from \"conformity\" (Big Brother).[39] These images were an allusion to George Orwell's noted novel, Nineteen Eighty-Four, which described a dystopian future ruled by a televised \"Big Brother.\"\n",
    "For a special post-election edition of Newsweek in November 1984, Apple spent more than US$2.5 million to buy all 39 of the advertising pages in the issue.[40] Apple also ran a \"Test Drive a Macintosh\" promotion, in which potential buyers with a credit card could take home a Macintosh for 24 hours and return it to a dealer afterwards. While 200,000 people participated, dealers disliked the promotion, the supply of computers was insufficient for demand, and many were returned in such a bad shape that they could no longer be sold. This marketing campaign caused CEO John Sculley to raise the price from US$1,995 (equivalent to $4,700 in 2017) to US$2,495 (equivalent to $5,900 in 2017).[41]\n",
    "Two days after the 1984 ad aired, the Macintosh went on sale. It came bundled with two applications designed to show off its interface: MacWrite and MacPaint. Although the Mac garnered an immediate, enthusiastic following, it was too radical for some, who labeled it a mere \"toy\". Because the machine was entirely designed around the GUI, existing text-mode and command-driven applications had to be redesigned and the programming code rewritten; this was a challenging undertaking that many software developers shied away from, and resulted in an initial lack of software for the new system. In April 1984 Microsoft's MultiPlan migrated over from MS-DOS, followed by Microsoft Word in January 1985.[42] In 1985, Lotus Software introduced Lotus Jazz after the success of Lotus 1-2-3for the IBM PC, although it was largely a flop.[43] Apple introduced Macintosh Office the same year with the lemmings ad, infamous for insulting potential customers. It was not successful.[41]\n",
    "Macintosh also spawned the concept of Mac evangelism which was pioneered by Apple employee, and later Apple Fellow, Guy Kawasaki.[citation needed]\n",
    "Despite initial marketing difficulties, the Macintosh brand was eventually a success for Apple. This was due to its introduction of desktop publishing (and later computer animation) through Apple's partnership with Adobe Systems which introduced the laser printer and Adobe PageMaker. Indeed, the Macintosh would become known as the de facto platform for many industries including cinema, music, advertising, publishing and the arts.\n",
    "Sculley and Jobs' visions for the company greatly differed. The former favored open architecture computers like the Apple II, sold to education, small business, and home markets less vulnerable to IBM. Jobs wanted the company to focus on the closed architecture Macintosh as a business alternative to the IBM PC. President and CEO Sculley had little control over Chairman of the Board Jobs' Macintosh division; it and the Apple II division operated like separate companies, duplicating services.[44] Although its products provided 85% of Apple's sales in early 1985, the company's January 1985 annual meeting did not mention the Apple II division or employees. Many left, including Wozniak, who stated that the company had \"been going in the wrong direction for the last five years\" and sold most of his stock.[45]\n",
    "The Macintosh's failure to defeat the PC strengthened Sculley's position in the company. In June 1985, the board of directors sided with Sculley and Jobs was stripped of all duties. Jobs, while taking the position of Chairman of the firm, had no influence over Apple's direction and subsequently resigned. Sculley reorganized the company, unifying sales and marketing in one division and product operations and development in another.[46][44] In a show of defiance at being set aside by Apple Computer, Jobs sold all but one of his 6.5 million shares in the company for $70 million. Jobs then acquired the visual effects house, Pixar for $5M (equivalent to $11,200,000 in 2017). He also went on to found NeXT Inc., a computer company that built machines with futuristic designs and ran the UNIX-derived NeXTstep operating system. NeXTSTEP would eventually be developed into Mac OS X. While not a commercial success, due in part to its high price, the NeXT computer would introduce important concepts to the history of the personal computer (including serving as the initial platform for Tim Berners-Lee as he was developing the World Wide Web). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Jocelyn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Jocelyn', 'NNP')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"My name is Jocelyn\"\n",
    "\n",
    "token = nltk.word_tokenize(sentence)\n",
    "print(token)\n",
    "\n",
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n"
     ]
    }
   ],
   "source": [
    "# We can get more details about any POS tag using help funciton of NLTK as follows.\n",
    "#nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset(\"PRP$\")\n",
    "\n",
    "\n",
    "# The most popular tag set is Penn Treebank tagset.\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "156\n",
      "['Apple', 'Inc.', ',', 'formerly', 'Apple', 'Computer', ',', 'Inc.', ',', 'is', 'a', 'multinational', 'corporation', 'that', 'creates', 'consumer', 'electronics', ',', 'personal', 'computers', ',', 'servers', ',', 'and', 'computer', 'software', ',', 'and', 'is', 'a', 'digital', 'distributor', 'of', 'media', 'content', '.']\n",
      "['The', 'company', 'also', 'has', 'a', 'chain', 'of', 'retail', 'stores', 'known', 'as', 'Apple', 'Stores', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the apple_text article into words\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = nltk.sent_tokenize(apple_text)\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "print(len(token_sentences))\n",
    "print(token_sentences[0])\n",
    "print(token_sentences[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "156\n",
      "[('Apple', 'NNP'), ('Inc.', 'NNP'), (',', ','), ('formerly', 'RB'), ('Apple', 'NNP'), ('Computer', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('is', 'VBZ'), ('a', 'DT'), ('multinational', 'JJ'), ('corporation', 'NN'), ('that', 'WDT'), ('creates', 'VBZ'), ('consumer', 'NN'), ('electronics', 'NNS'), (',', ','), ('personal', 'JJ'), ('computers', 'NNS'), (',', ','), ('servers', 'NNS'), (',', ','), ('and', 'CC'), ('computer', 'NN'), ('software', 'NN'), (',', ','), ('and', 'CC'), ('is', 'VBZ'), ('a', 'DT'), ('digital', 'JJ'), ('distributor', 'NN'), ('of', 'IN'), ('media', 'NNS'), ('content', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('company', 'NN'), ('also', 'RB'), ('has', 'VBZ'), ('a', 'DT'), ('chain', 'NN'), ('of', 'IN'), ('retail', 'JJ'), ('stores', 'NNS'), ('known', 'VBN'), ('as', 'IN'), ('Apple', 'NNP'), ('Stores', 'NNPS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# tag each tokenized sentence into parts of speech: pos_sentences\n",
    "\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
    "print(type(pos_sentences))\n",
    "print(len(pos_sentences))\n",
    "\n",
    "print(pos_sentences[0])\n",
    "print(pos_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
    "print(type(chunked_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            print(chunk)\n",
    "            print(chunk.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Apple/NNP Inc./NNP)\n",
      "NE\n",
      "(',', ',')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-0e12a8e49e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        print(chunk)\n",
    "        print(chunk.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import collections\n",
    "\n",
    "ner_categories = collections.defaultdict(int)\n",
    "\n",
    "# fill the dictionary with values for each of the keys, they will represent he label()\n",
    "\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
    "print(type(chunked_sentences))\n",
    "\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\")  and chunk.label() == \"NE\":\n",
    "            ner_categories[chunk.label()]+=1\n",
    "            print(chunk.label())\n",
    "\n",
    "# for the pie chart labels, create a list called labels from the keys of ner_categories\n",
    "# which can be accessed using .keys()\n",
    "\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# create a list of the values\n",
    "\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ParserI.parse_sents.<locals>.<genexpr> at 0x00000202DB50D258>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
    "\n",
    "chunked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.EntityRecognizer at 0x202e1095938>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# run in prompt\n",
    "# python -m spacy download en\n",
    "# make sure linking did not fail - else just run the prompt as admin (right click)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "nlp.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "15\n",
      "(1609, English, Henry Hudson, New York Harbor, the Northwest Passage, Orient, the Dutch East India Company, \n",
      ", Dutch, the North River, the Hudson River, first, Hudson, Mauritius, Maurice)\n",
      "-----------\n",
      "1609 DATE\n",
      "English LANGUAGE\n",
      "Henry Hudson PERSON\n",
      "New York Harbor LOC\n",
      "the Northwest Passage GPE\n",
      "Orient LOC\n",
      "the Dutch East India Company ORG\n",
      "\n",
      " GPE\n",
      "Dutch NORP\n",
      "the North River LOC\n",
      "the Hudson River LOC\n",
      "first ORDINAL\n",
      "Hudson LOC\n",
      "Mauritius PERSON\n",
      "Maurice PERSON\n"
     ]
    }
   ],
   "source": [
    "test = nlp(\"\"\"In 1609, the English explorer Henry Hudson rediscovered the New York Harbor while searching for the Northwest Passage to the Orient for the Dutch East India Company.\n",
    "He proceeded to sail up what the Dutch would name the North River (now the Hudson River), named first by Hudson as the Mauritius after Maurice, Prince of Orange.\n",
    "\"\"\")\n",
    "\n",
    "# we defined an english nlp object, then we used it to load a text into it\n",
    "# we can see that we can refrer using an index to the relavent entity and find its label\n",
    "\n",
    "print(type(test.ents))\n",
    "print(len(test.ents))\n",
    "print(test.ents)\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "for i in range(0,len(test.ents)):\n",
    "    print(test.ents[i], test.ents[i].label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG \n",
      "Apple Inc.\n",
      "ORG Apple Computer, Inc.\n",
      "ORG Apple Stores\n",
      "ORG Apple\n",
      "ORG iPhone\n",
      "ORG iPad\n",
      "ORG Macintosh\n",
      "PERSON Steve Jobs\n",
      "PERSON Steve Wozniak\n",
      "ORG Apple Computer\n",
      "DATE April 1, 1976,[1\n",
      "DATE January 3, 1977,[2\n",
      "GPE Cupertino\n",
      "GPE California\n",
      "GPE \n",
      "\n",
      "DATE more than three decades\n",
      "ORG Apple Computer\n",
      "PRODUCT the Apple II\n",
      "ORG Macintosh\n",
      "ORG Power Mac\n",
      "DATE the 1990s\n",
      "PERSON Jobs\n",
      "DATE 1985\n",
      "ORG Apple\n",
      "DATE 1996\n",
      "ORG NeXT\n",
      "PERSON Jobs\n",
      "DATE 1998\n",
      "GPE \n",
      "\n",
      "ORG iPod\n",
      "DATE 2001\n",
      "ORG iTunes Music Storein 2003\n",
      "ORG Apple\n",
      "WORK_OF_ART Computer\n",
      "DATE 2007\n",
      "ORG iPhone\n",
      "ORG the iPod Touch\n",
      "ORG iPad\n",
      "DATE 30 June 2015\n",
      "ORG Apple\n",
      "PRODUCT capitalization,[6\n",
      "MONEY US$1 trillion\n",
      "DATE August 2\n",
      "ORG 2018.[7\n",
      "ORG Apple\n",
      "DATE annual\n",
      "DATE 2010\n",
      "MONEY US$65 billion\n",
      "MONEY US$127.8 billion\n",
      "CARDINAL 2011[8\n",
      "MONEY $156 billion\n",
      "CARDINAL 2012.[9\n",
      "PERSON Steve Jobs\n",
      "PERSON Steve Wozniak\n",
      "ORG Reed College\n",
      "ORG UC Berkeley\n",
      "DATE 1975\n",
      "ORG Wozniak\n",
      "ORG Call Computer\n",
      "PERSON Alex Kamradt\n",
      "PERSON Jobs\n",
      "GPE Wozniak\n",
      "CARDINAL two\n",
      "PERSON Satori\n",
      "DATE 1986\n",
      "PERSON Theodore Roszak\n",
      "ORG the Apple Computer\n",
      "LOC the West Coast\n",
      "CARDINAL two\n",
      "ORG Steves\n",
      "GPE \n",
      "\n",
      "DATE 1975\n",
      "ORG Wozniak\n",
      "ORG the Homebrew Computer Club\n",
      "ORG the Altair 8800\n",
      "ORG IMSAI\n",
      "GPE \n",
      "\n",
      "MONEY $179 Intel\n",
      "MONEY 815\n",
      "DATE 2017\n",
      "MONEY 170\n",
      "ORG Motorola 6800\n",
      "MONEY 774\n",
      "DATE 2017\n",
      "PERSON Wozniak\n",
      "CARDINAL 6800\n",
      "DATE the day\n",
      "GPE \n",
      "\n",
      "ORG MOS Technology\n",
      "MONEY 20\n",
      "MONEY 86\n",
      "DATE 2017\n",
      "DATE 6502\n",
      "DATE 1976\n",
      "GPE Wozniak\n",
      "DATE 6502\n",
      "DATE 6800\n",
      "LOC Silicon Valley\n",
      "GPE Wozniak\n",
      "CARDINAL 6800\n",
      "GPE \n",
      "\n",
      "PERSON Wozniak\n",
      "ORG Homebrew Computer Club\n",
      "GPE Wozniak\n",
      "PERSON Jobs\n",
      "GPE \n",
      "\n",
      "CARDINAL two\n",
      "ORG Steves\n",
      "DATE 1971\n",
      "PERSON Bill Fernandez\n",
      "GPE Wozniak\n",
      "DATE 16-year-old\n",
      "PERSON Jobs\n",
      "GPE Wozniak\n",
      "CARDINAL one\n",
      "CARDINAL several hundred\n",
      "PERSON Jobs\n",
      "GPE Wozniak\n",
      "GPE \n",
      "\n",
      "PERSON Jobs\n",
      "ORG The Byte Shop\n",
      "PERSON Paul Terrell\n",
      "CARDINAL 50\n",
      "GPE US\n",
      "MONEY 500\n",
      "MONEY 2,200\n",
      "CARDINAL 2017).[12]Jobs\n",
      "ORG the Byte Shop to Cramer Electronics\n",
      "ORG the Apple I Computer\n",
      "PERSON Jobs\n",
      "ORG Byte Shop\n",
      "CARDINAL 50\n",
      "ORG COD\n",
      "DATE 30-day\n",
      "ORG Terrell\n",
      "ORG the Byte Shop\n",
      "GPE \n",
      "\n",
      "PERSON Paul Terrell\n",
      "ORG IEEE\n",
      "FAC Asilomar\n",
      "GPE Pacific Grove\n",
      "GPE Jobs\n",
      "ORG Terrell\n",
      "PERSON Jobs\n",
      "CARDINAL two\n",
      "DATE day\n",
      "ORG Terrell\n",
      "PERSON Steve Jobs\n",
      "MONEY multimillion-dollar\n",
      "CARDINAL one\n",
      "GPE \n",
      "\n",
      "CARDINAL One\n",
      "CARDINAL 60\n",
      "ORDINAL second\n",
      "DATE that era\n",
      "ORG Apple\n",
      "ORG ROM\n",
      "PERSON Paul Terrell\n",
      "GPE Wozniak\n",
      "CARDINAL 1200\n",
      "GPE Wozniak\n",
      "GPE \n",
      "\n",
      "PERSON Ronald Wayne\n",
      "CARDINAL two\n",
      "GPE Wozniak\n",
      "PERSON Jobs\n",
      "PERSON Jobs\n",
      "GPE Wozniak\n",
      "PERSON Wayne\n",
      "ORG the Byte Shop\n",
      "ORG Terrell\n",
      "PERSON Jobs\n",
      "PERSON Jobs\n",
      "PERSON Mike Markkula\n",
      "MONEY 250,000\n",
      "MONEY 1,080,000\n",
      "DATE 2017\n",
      "PERSON Jobs\n",
      "GPE Wozniak\n",
      "PERSON Wayne\n",
      "ORG Apple Computer\n",
      "DATE April 1, 1976\n",
      "PERSON Wayne\n",
      "DATE four years earlier\n",
      "CARDINAL two\n",
      "ORG Steves\n",
      "ORG Apple\n",
      "ORG Atari\n",
      "ORG Apple Computer\n",
      "ORG Atari\n",
      "PERSON Jobs\n",
      "GPE Oregon\n",
      "CARDINAL one summer.[16\n",
      "CARDINAL 200\n",
      "ORG Apple\n",
      "GPE \n",
      "\n",
      "ORG Wozniak\n",
      "ORG Apple\n",
      "GPE Wozniak\n",
      "PRODUCT the Apple II\n",
      "CARDINAL two\n",
      "ORG Steves\n",
      "ORDINAL first\n",
      "LOC West Coast Computer Faire\n",
      "DATE April 16 and 17, 1977\n",
      "DATE the first day\n",
      "PERSON Jobs\n",
      "PRODUCT the Apple II\n",
      "NORP Japanese\n",
      "PERSON Toshio Mizushima\n",
      "ORDINAL first\n",
      "ORG Apple\n",
      "GPE Japan\n",
      "GPE \n",
      "\n",
      "PERSON Jobs\n",
      "ORG Apple\n",
      "ORG The Byte Shop\n",
      "CARDINAL one\n",
      "PERSON Jerry Manock\n",
      "PRODUCT the Apple II\n",
      "DATE 1977\n",
      "CARDINAL one\n",
      "CARDINAL three\n",
      "DATE 1977\n",
      "ORG Trinity\n",
      "CARDINAL two\n",
      "PERSON PET\n",
      "ORG the Tandy Corporation TRS-80).[17\n",
      "CARDINAL Millions\n",
      "DATE the 1980s\n",
      "PRODUCT Apple II\n",
      "ORG Apple\n",
      "ORG Apple IIGS\n",
      "DATE nearly two decades\n",
      "GPE \n",
      "\n",
      "PRODUCT the Apple II\n",
      "PERSON Visicalc\n",
      "ORG Apple\n",
      "PRODUCT The Apple III\n",
      "PERSON The Apple III\n",
      "DATE May 19, 1980\n",
      "GPE \n",
      "\n",
      "PRODUCT The Apple III\n",
      "PERSON Steve Jobs\n",
      "GPE \n",
      "\n",
      "ORG Apple\n",
      "PRODUCT III\n",
      "ORG Apple\n",
      "QUANTITY six inches\n",
      "GPE \n",
      "\n",
      "CARDINAL Thousands\n",
      "ORG Apple\n",
      "PRODUCT III\n",
      "DATE 1983\n",
      "GPE \n",
      "\n",
      "DATE July 1980\n",
      "ORG Kilobaud Microcomputing\n",
      "PERSON Wayne Green\n",
      "ORG Apple\n",
      "ORDINAL sale.\"[18\n",
      "DATE August\n",
      "ORG the Financial Times\n",
      "ORG \n",
      "Apple Computer\n",
      "NORP Californian\n",
      "DATE later this year\n",
      "GPE U.S.\n",
      "DATE about five years ago\n",
      "ORDINAL second\n",
      "ORG the Radio Shack\n",
      "ORG Tandy\n",
      "GPE \n",
      "\n",
      "DATE December 12, 1980\n",
      "ORG Apple\n",
      "ORG the Initial Public Offering\n",
      "ORG Apple\n",
      "ORG IPO\n",
      "ORG Ford Motor Company\n",
      "DATE 1956\n",
      "CARDINAL about 300\n",
      "CARDINAL billions\n",
      "GPE \n",
      "\n",
      "DATE January 1981\n",
      "ORG Apple\n",
      "ORDINAL first\n",
      "FAC the Flint Center\n",
      "ORG De Anza College\n",
      "TIME 15 minutes\n",
      "DATE days\n",
      "DATE months\n",
      "ORG IPO\n",
      "PERSON Steve Jobs\n",
      "GPE \n",
      "\n",
      "DATE August 1981\n",
      "ORG Apple\n",
      "CARDINAL three\n",
      "ORG Radio Shack\n",
      "ORG IBM\n",
      "DATE that month\n",
      "ORG the IBM PC,[22\n",
      "ORG Apple\n",
      "ORG IBM\n",
      "CARDINAL one\n",
      "CARDINAL a couple of hundred\n",
      "ORG Apple\n",
      "CARDINAL five\n",
      "GPE US\n",
      "PRODUCT The Apple II\n",
      "CARDINAL more than 250,000\n",
      "CARDINAL hundreds\n",
      "CARDINAL at least ten\n",
      "CARDINAL ten\n",
      "ORG BYTE\n",
      "DATE 1984\n",
      "GPE \n",
      "\n",
      "CARDINAL two\n",
      "ORG Apple\n",
      "ORG Apple\n",
      "ORG Nature\n",
      "ORG Apple\n",
      "GPE \n",
      "\n",
      "ORG Apple\n",
      "ORG Apple\n",
      "CARDINAL two\n",
      "CARDINAL two\n",
      "MONEY billion-dollar\n",
      "NORP American\n",
      "ORG Apple\n",
      "PERSON David\n",
      "ORG IBM\n",
      "PERSON Goliath\n",
      "GPE \n",
      "\n",
      "ORG Apple\n",
      "PRODUCT The Apple III\n",
      "ORG Apple\n",
      "ORG Apple\n",
      "ORG The Wall Street Journal\n",
      "ORG IBM\n",
      "ORG Microsoft\n",
      "PERSON Bill Gates\n",
      "ORG Apple\n",
      "DATE the day\n",
      "ORG IBM\n",
      "DATE a full year\n",
      "CARDINAL happened\".[22\n",
      "DATE 1983\n",
      "WORK_OF_ART the Apple II\n",
      "DATE 1984\n",
      "ORG IBM\n",
      "MONEY $4 billion\n",
      "DATE annual\n",
      "ORG Apple\n",
      "CARDINAL three\n",
      "ORG Apple II\n",
      "ORG Fortune\n",
      "PERCENT 56%\n",
      "NORP American\n",
      "ORG IBM\n",
      "PERCENT 16%\n",
      "ORG II\n",
      "ORG \n",
      "Apple Computer's\n",
      "ORG Apple\n",
      "PRODUCT III\n",
      "PERSON Lisa\n",
      "MONEY 1,000,000\n",
      "ORG Xerox\n",
      "ORG Apple Computer\n",
      "DATE three days\n",
      "ORG PARC\n",
      "ORG PARC\n",
      "ORG Apple Computer's\n",
      "ORDINAL first\n",
      "ORG GUIcomputer\n",
      "ORG the Apple Lisa.[31][32][33][34]\n",
      "\n",
      "ORDINAL first\n",
      "ORG Apple\n",
      "ORG WIMP\n",
      "DATE months\n",
      "ORG Apple\n",
      "PERSON Lisa\n",
      "GPE \n",
      "\n",
      "PERSON Lisa\n",
      "DATE 1983\n",
      "GPE US\n",
      "MONEY 9,995\n",
      "MONEY 24,600\n",
      "DATE 2017\n",
      "PERSON Lisa\n",
      "GPE \n",
      "\n",
      "ORG Macintosh\n",
      "PRODUCT 128k\n",
      "DATE October 1983\n",
      "MONEY US$1.5 million\n",
      "WORK_OF_ART 1984\n",
      "MONEY 3,500,000\n",
      "DATE 2017\n",
      "PERSON Ridley Scott\n",
      "DATE the third quarter\n",
      "EVENT Super Bowl\n",
      "ORG XVIII\n",
      "DATE January 22\n",
      "CARDINAL 1984,[36\n",
      "ORG Macintosh\n",
      "WORK_OF_ART Picasso\n",
      "ORG Apple\n",
      "ORG Macintosh\n",
      "PERSON George Orwell's\n",
      "ORG Nineteen Eighty-Four\n",
      "GPE \n",
      "\n",
      "ORG Newsweek\n",
      "DATE November 1984\n",
      "ORG Apple\n",
      "MONEY more than US$2.5 million\n",
      "CARDINAL 39\n",
      "ORG Apple\n",
      "WORK_OF_ART Test Drive a Macintosh\n",
      "ORG Macintosh\n",
      "TIME 24 hours\n",
      "CARDINAL 200,000\n",
      "PERSON John Sculley\n",
      "MONEY 1,995\n",
      "MONEY 4,700\n",
      "DATE 2017\n",
      "MONEY 2,495\n",
      "MONEY 5,900\n",
      "CARDINAL 2017).[41\n",
      "GPE \n",
      "\n",
      "DATE Two days\n",
      "DATE 1984\n",
      "ORG Macintosh\n",
      "CARDINAL two\n",
      "ORG MacWrite\n",
      "ORG MacPaint\n",
      "PERSON Mac\n",
      "ORG GUI\n",
      "DATE April 1984\n",
      "ORG Microsoft\n",
      "ORG MultiPlan\n",
      "ORG MS-DOS\n",
      "ORG Microsoft\n",
      "PRODUCT Word\n",
      "DATE January\n",
      "DATE 1985\n",
      "ORG Lotus Software\n",
      "ORG Lotus Jazz\n",
      "ORG Lotus\n",
      "PRODUCT 1-2-3for\n",
      "ORG IBM\n",
      "ORG Apple\n",
      "ORG Macintosh Office\n",
      "DATE the same year\n",
      "GPE \n",
      "\n",
      "ORG Macintosh\n",
      "PERSON Mac\n",
      "ORG Apple\n",
      "ORG Guy Kawasaki.[citation\n",
      "GPE \n",
      "\n",
      "ORG Macintosh\n",
      "ORG Apple\n",
      "ORG Apple\n",
      "ORG Adobe Systems\n",
      "ORG Adobe PageMaker\n",
      "ORG Macintosh\n",
      "GPE \n",
      "\n",
      "ORG Sculley\n",
      "PERSON Jobs\n",
      "PRODUCT the Apple II\n",
      "ORG IBM\n",
      "PERSON Jobs\n",
      "ORG Macintosh\n",
      "ORG IBM\n",
      "PERSON Sculley\n",
      "ORG the Board Jobs'\n",
      "ORG Macintosh\n",
      "ORG Apple II\n",
      "PERCENT 85%\n",
      "ORG Apple\n",
      "DATE early 1985\n",
      "DATE January 1985\n",
      "DATE annual\n",
      "ORG Apple II\n",
      "GPE Wozniak\n",
      "DATE the last five years\n",
      "GPE \n",
      "\n",
      "ORG Macintosh\n",
      "ORG Sculley\n",
      "DATE June 1985\n",
      "ORG Sculley\n",
      "PERSON Jobs\n",
      "PERSON Jobs\n",
      "ORG Apple\n",
      "CARDINAL one\n",
      "ORG Apple Computer\n",
      "PERSON Jobs\n",
      "CARDINAL one\n",
      "CARDINAL 6.5 million\n",
      "MONEY $70 million\n",
      "PERSON Jobs\n",
      "ORG Pixar for\n",
      "MONEY $5M\n",
      "MONEY 11,200,000\n",
      "DATE 2017\n",
      "ORG NeXT Inc.\n",
      "ORG NeXTSTEP\n",
      "PRODUCT Mac OS X.\n",
      "ORG NeXT\n",
      "PERSON Tim Berners-Lee\n",
      "ORG the World Wide Web\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add arguments to minimize execution times\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "apple_nlp = nlp(apple_text)\n",
    "\n",
    "# print all the found entities and their labels\n",
    "for ent in apple_nlp.ents:\n",
    "    print(ent.label_, ent.text)\n",
    "    \n",
    "# we can see that spacy has additional entity types compared to nltk:\n",
    "# NORP, CARDINAL, MONEY, WORK OF ART, LANGUAGE, EVENT..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polyglot can also detect the relavent language automatically once we begin\n",
    "\n",
    "text = \"\"\"\n",
    "\n",
    "תניהו נולד בתל אביב וגדל בירושלים. הוא השני מבין שלושה בנים של צילה לבית סגל (1912–2000) ופרופ' בנציון נתניהו (1910–2012). בסוף שנות ה-50 וה-60 חייתה המשפחה לסירוגין בישראל ובארצות הברית. הוא החל ללמוד בתיכון ליד האוניברסיטה, עד שמשפחתו עברה לארצות הברית בשנים 1963–1969, וגרה בצ'לטנהם, פרוור של פילדלפיה שבפנסילבניה, שם למד נתניהו בבית הספר התיכון המקומי והיה פעיל במועדון הדיבייט.\n",
    "\n",
    "ב-1967, עם סיום לימודיו בתיכון, שב נתניהו לישראל כדי להתגייס לצה\"ל. שרת כלוחם וכמפקד בסיירת מטכ\"ל. ביחידה עבר מסלול הכשרה כלוחם וכמפקד כיתה, תחת פיקודו של עמירם לוין[1] והשתתף בפעולות מיוחדות בעומק קווי האויב. בין היתר, השתתף במבצע תשורה, במבצע תופת ובמבצע בולמוס 4 במהלך מלחמת ההתשה[2], במהלכו כמעט ונהרג, לאחר שהצבא המצרי פתח באש על כוח היחידה והקומנדו הימי, בעת שצלח את תעלת סואץ על גבי סירות גומי, בדרכו לעבר הגדה המערבית, שהייתה בשליטת כוחות מצריים[3]. בהמשך, סיים קורס קציני חי\"ר בהצטיינות[4] ומונה למפקד צוות ביחידה[5]. כמפקד צוות השתתף נתניהו, בין היתר, במבצע ההשתלטות על מטוס \"סבנה\", ב-8 במאי 1972, שעליו פיקד אהוד ברק, מפקד היחידה דאז. נתניהו נפצע בפעולה זו מפליטת כדור של חברו ליחידה. בפעם אחרת, בעת ביצוע פעולה עלומה בסוריה, נתקע הצוות שעליו פיקד בדרך והלוחמים סבלו מתשישות ומקור. מפקד פלוגתו, עוזי דיין, חש לעזרתם וחילץ אותם[6]. בקיץ 1972, השתחרר נתניהו מצה\"ל בדרגת סרן. הוא הוסיף לשרת במילואים בסיירת לפחות עד שנת 1981[7].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polyglot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-12bb905eae70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mptext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mptext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhint_language_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'he'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'polyglot'"
     ]
    }
   ],
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "ptext = Text(text)\n",
    "\n",
    "ptext.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spam_df = pd.read_csv(\"C:/python_data/spam/spam.csv\", encoding = 'latin-1')\n",
    "\n",
    "spam_df = spam_df[[\"v1\",\"v2\"]]\n",
    "\n",
    "spam_df.rename(columns={'v1': 'spam', 'v2':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "   spam                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "6   ham  Even my brother is not like to speak with me. ...\n",
      "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "8  spam  WINNER!! As a valued network customer you have...\n",
      "9  spam  Had your mobile 11 months or more? U R entitle...\n"
     ]
    }
   ],
   "source": [
    "print(spam_df.shape)\n",
    "\n",
    "print(spam_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3077               Okay but i thought you were the expert\n",
       "92      Smile in Pleasure Smile in Pain Smile when tro...\n",
       "1333           Oh... Icic... K lor, den meet other day...\n",
       "3676          Great! So what attracts you to the brothas?\n",
       "4190    Each Moment in a day,has its own value-Morning...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train and test sets\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# store the label in a series\n",
    "y = spam_df.spam\n",
    "print(type(y))\n",
    "\n",
    "# create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_df[\"text\"], y,\n",
    "                                                   test_size=0.33, random_state=1)\n",
    "\n",
    "print(type(X_train))\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3733, 6782)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(1839, 6782)\n",
      "<class 'list'>\n",
      "6782\n",
      "['00', '000', '008704050406', '0121', '0125698789', '02', '0207', '02072069400', '02073162414', '02085076972', '021', '03', '04', '0430', '05', '050703', '0578', '06', '07', '07008009200', '07046744435', '07090298926', '07099833605', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '0776xxxxxxx', '07781482378', '07786200117', '077xxx', '078', '07801543489', '07808247860', '07808726822', '07815296484', '07821230901', '07880867867', '0789xxxxxxx', '07946746291', '07973788240', '07xxxxxxxxx', '08', '0800', '08000407165', '08000776320', '08000839402', '08000930705', '08000938767', '08001950382', '08002888812', '08002986030', '08002986906', '08006344447', '0808', '08081560665', '0825', '083', '0844', '08448714184', '0845', '08452810071', '08452810073', '08452810075over18', '0870', '08700435505150p', '08700469649', '08700621170150p', '08701213186', '08701237397', '08701417012', '08701417012150p', '0870141701216', '087016248', '08701752560', '087018728737', '0870241182716', '08702840625', '08704439680ts', '08706091795', '08707500020', '08707509020', '08707808226', '08708034412', '08708800282', '08709501522', '0871', '087104711148', '08712101358', '08712103738', '0871212025016', '08712300220', '08712317606', '08712400200', '08712400602450p', '08712400603', '08712402050', '08712402578', '08712402779', '08712402972', '08712405020', '08712405022', '08712460324', '08712466669', '0871277810710p', '0871277810810', '08714342399', '087147123779am', '08714712379', '08714712388', '08714712412', '08714714011', '08715203028', '08715203649', '08715203685', '08715203694', '08715705022', '0871750', '08717509990', '08717890890å', '08717895698', '08717898035', '08718720201', '08718723815', '08718725756', '08718726270', '087187262701', '08718726971', '08718726978', '08718727868', '08718727870', '08718727870150ppm', '08718730666', '08718738001', '08718738034', '08719180248', '08719181259', '08719181503', '08719181513', '08719839835', '08719899229', '08719899230', '09', '09041940223', '09050000332', '09050000460', '09050000555', '09050000928', '09050001295', '09050001808', '09050002311', '09050003091', '09050005321', '09050090044', '09050280520', '09053750005', '09056242159', '09057039994', '09058091854', '09058094455', '09058094565', '09058094583', '09058094594', '09058094597', '09058094599', '09058095107', '09058095201', '09058097189', '09058099801', '09061104283', '09061209465', '09061213237', '09061221061', '09061221066', '09061701444', '09061701461', '09061701851', '09061701939', '09061702893', '09061743386', '09061743806', '09061743810', '09061743811', '09061744553', '09061749602', '09061790121', '09061790126', '09063440451', '09063458130', '0906346330', '09064011000', '09064012103', '09064012160', '09064015307', '09064017295', '09064017305', '09064018838', '09064019014', '09065171142', '09065174042', '09065989182', '09066350750', '09066358152', '09066361921', '09066362206', '09066362220', '09066362231', '09066364311', '09066364349', '09066364589', '09066368327', '09066368753', '09066380611', '09066612661', '09066649731from', '09066660100', '09071512433', '09071517866', '09090204448', '09094100151', '09094646631', '09094646899', '09095350301', '09096102316', '09099726395', '09099726553', '09111030116', '09111032124', '09701213186', '0a', '0quit', '10', '100', '1000', '1000s', '100p', '100percent', '1013', '1030', '10am', '10k', '10p', '10ppm', '10th', '11', '113', '1131', '114', '116', '118p', '11mths', '11pm', '12', '120p', '121', '1225', '123', '1250', '125gift', '128', '12hours', '12hrs', '12mths', '13', '130', '1327', '139', '14', '1405', '140ppm', '145', '1450', '15', '150', '1500', '150p', '150p16', '150pm', '150ppermesssubscription', '150ppm', '150ppmpobox10183bhamb64xe', '150ppmsg', '150pw', '151', '15541', '15pm', '16', '165', '1680', '169', '177', '18', '1843', '18p', '18yrs', '195', '1956669', '1apple', '1b6a5ecef91ff9', '1cup', '1da', '1er', '1im', '1lemon', '1mega', '1st', '1st4terms', '1stchoice', '1stone', '1thing', '1tulsi', '1win150ppmx3', '1winaweek', '1winawk', '1x150p', '1yf', '20', '200', '2000', '2003', '2004', '2005', '2006', '2007', '200p', '2025050', '20p', '21', '21870000', '21st', '220', '220cm2', '2309', '23f', '23g', '24', '24hrs', '24m', '24th', '25', '250', '250k', '255', '25p', '26', '27', '28', '2814032', '28days', '28th', '28thfeb', '29', '2b', '2bold', '2day', '2ez', '2find', '2getha', '2geva', '2go', '2lands', '2marrow', '2moro', '2morow', '2morrow', '2mrw', '2nd', '2nhite', '2nights', '2nite', '2optout', '2px', '2rcv', '2stop', '2stoptxt', '2u', '2waxsto', '2wks', '2wu', '2years', '2yr', '2yrs', '30', '300', '3000', '300p', '3030', '30ish', '30pm', '30pp', '31', '3100', '310303', '32', '32000', '3230', '32323', '326', '33', '350', '3510i', '35p', '36504', '3680', '373', '3750', '37819', '3aj', '3d', '3days', '3g', '3gbp', '3hrs', '3lp', '3mins', '3optical', '3pound', '3qxj9', '3rd', '3ss', '3uz', '3xå', '40', '400', '400mins', '402', '4041', '40411', '40533', '40gb', '40mph', '41685', '41782', '4217', '430', '434', '440', '4403ldnw1a7rw18', '44345', '447797706009', '447801259231', '449071512431', '45', '450', '450p', '450pw', '45239', '45pm', '47', '4742', '48', '4882', '49557', '4a', '4d', '4eva', '4few', '4fil', '4get', '4give', '4got', '4info', '4jx', '4msgs', '4mths', '4qf2', '4t', '4th', '4the', '4txt', '4u', '4utxt', '4w', '4ward', '4wrd', '4xx26', '4years', '50', '500', '5000', '505060', '50award', '50ea', '50gbp', '50p', '50perweeksub', '50perwksub', '50pm', '50pmmorefrommobile2bremoved', '50rcvd', '515', '5226', '526', '528', '530', '54', '542', '5digital', '5free', '5ish', '5k', '5min', '5mls', '5p', '5pm', '5th', '5wb', '5we', '5wq', '5years', '600', '6031', '6089', '60p', '61200', '61610', '62220cncl', '62468', '62735', '630', '63miles', '645', '6669', '674', '68866', '69101', '69669', '69696', '69698', '69855', '69866', '69911', '69969', '69988', '6hl', '6hrs', '6ish', '6missed', '6months', '6pm', '6th', '6times', '6wu', '6zf', '700', '71', '7250i', '730', '731', '75', '750', '7548', '7634', '7684', '77', '78', '786', '7876150ppm', '79', '7am', '7cfca1a', '7ish', '7pm', '7th', '7ws', '7zs', '80', '800', '80062', '8007', '80086', '80155', '80182', '8027', '80488', '80608', '8077', '80878', '81010', '81151', '81303', '81618', '82050', '820554ad0a1705572711', '82242', '82277', '82324', '83039', '83049', '83110', '83118', '83222', '83338', '83355', '83370', '83383', '83435', '83600', '83738', '84025', '84128', '84199', '84484', '85', '85023', '85069', '86021', '861', '864233', '86688', '86888', '87021', '87066', '87077', '87121', '87131', '872', '87239', '87575', '88039', '88066', '88088', '88222', '88600', '8883', '88877', '88888', '89080', '89105', '89123', '89545', '89555', '89693', '89938', '8am', '8ball', '8pm', '8th', '8wp', '900', '9061100010', '9280114', '930', '9307622', '9755', '9758', '97n7qp', '98321561', '99', '9ae', '9am', '9ja', '9pm', '9t', '9th', '9yt', '____', 'a21', 'a30', 'aah', 'aaooooright', 'aathi', 'ab', 'abbey', 'abeg', 'abel', 'abi', 'ability', 'abiola', 'abj', 'able', 'abnormally', 'aboutas', 'abroad', 'absolutely', 'absolutly', 'abstract', 'abt', 'abta', 'aburo', 'abusers', 'ac', 'academic', 'acc', 'accent', 'accenture', 'accept', 'access', 'accessible', 'accidant', 'accident', 'accidentally', 'accommodation', 'accommodationvouchers', 'accomodate', 'accomodations', 'accordin', 'accordingly', 'account', 'accounting', 'accounts', 'accumulation', 'ache', 'acid', 'acl03530150pm', 'acnt', 'aco', 'acted', 'actin', 'action', 'activate', 'active', 'activities', 'actor', 'actual', 'actually', 'ad', 'adam', 'add', 'addamsfa', 'added', 'addicted', 'addie', 'adding', 'address', 'adds', 'adi', 'adjustable', 'admin', 'administrator', 'admirer', 'admission', 'adore', 'adoring', 'adress', 'ads', 'adsense', 'adult', 'adults', 'advance', 'adventure', 'advice', 'advise', 'advisors', 'affair', 'affairs', 'affection', 'affectionate', 'affections', 'affidavit', 'afford', 'afghanistan', 'afraid', 'african', 'aft', 'afternoon', 'afternoons', 'aftr', 'ag', 'agalla', 'age', 'age16', 'age23', 'agency', 'agent', 'agents', 'ages', 'aging', 'ago', 'agree', 'ah', 'aha', 'ahead', 'ahhh', 'ahmad', 'ahold', 'aid', 'aids', 'aig', 'aight', 'ain', 'aint', 'air', 'air1', 'airport', 'airtel', 'aiya', 'aiyah', 'aiyar', 'aiyo', 'ajith', 'ak', 'aka', 'al', 'alaipayuthe', 'album', 'alcohol', 'aldrine', 'alert', 'alerts', 'alex', 'alfie', 'algarve', 'algebra', 'algorithms', 'ali', 'alian', 'alibi', 'alive', 'allah', 'allalo', 'alle', 'allow', 'allowed', 'allows', 'alright', 'alrite', 'alter', 'alternative', 'alto18', 'aluable', 'alwa', 'alwys', 'amazing', 'ambitious', 'ambrith', 'american', 'ami', 'amigos', 'amk', 'amla', 'ammo', 'amore', 'amp', 'amplikater', 'amrca', 'amt', 'amused', 'amy', 'ana', 'analysis', 'anderson', 'andres', 'andros', 'angels', 'angry', 'animal', 'animation', 'anjie', 'anjola', 'anna', 'annie', 'anniversary', 'annoncement', 'announced', 'announcement', 'anot', 'ans', 'ansr', 'answer', 'answered', 'answerin', 'answering', 'answers', 'answr', 'antelope', 'antha', 'anthony', 'anti', 'anybody', 'anymore', 'anyones', 'anyplaces', 'anythiing', 'anythin', 'anythingtomorrow', 'anytime', 'anyways', 'aom', 'apart', 'apartment', 'apeshit', 'aphexåõs', 'apnt', 'apo', 'apologetic', 'apologise', 'app', 'apparently', 'appendix', 'applausestore', 'applebees', 'apples', 'application', 'apply', 'applying', 'appointments', 'appreciate', 'appreciated', 'approaches', 'approaching', 'appropriate', 'approved', 'approx', 'apps', 'appt', 'appy', 'april', 'apt', 'aquarius', 'ar', 'arab', 'arabian', 'archive', 'ard', 'area', 'aren', 'arent', 'aretaking', 'argh', 'argue', 'arguing', 'argument', 'aries', 'arm', 'armand', 'armenia', 'arms', 'arng', 'arnt', 'arrange', 'arrested', 'arrival', 'arrive', 'arrived', 'arrow', 'arsenal', 'art', 'artists', 'arts', 'arty', 'arul', 'arun', 'asa', 'asap', 'asda', 'ashes', 'ashwini', 'asia', 'asian', 'ask', 'askd', 'asked', 'askin', 'asking', 'asks', 'asleep', 'asp', 'ass', 'assessment', 'assistance', 'assume', 'assumed', 'asthere', 'astne', 'astrology', 'astronomer', 'asus', 'asusual', 'ate', 'atlanta', 'atlast', 'atm', 'atrocious', 'attach', 'attached', 'attempt', 'atten', 'attend', 'attended', 'attending', 'attention', 'attitude', 'attraction', 'attractive', 'attracts', 'atural', 'auction', 'audition', 'audrey', 'audrie', 'august', 'aunt', 'auntie', 'aunties', 'aunts', 'aunty', 'aust', 'australia']\n"
     ]
    }
   ],
   "source": [
    "# create bow\n",
    "# with sklearn bow can also remove stop words\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "print(type(count_vectorizer))\n",
    "print(count_vectorizer)\n",
    "\n",
    "\n",
    "# transfrom the training data using only the 'text' column\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "print(type(count_train))\n",
    "print(count_train.shape)\n",
    "\n",
    "\n",
    "# transform the test\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "print(type(count_test))\n",
    "print(count_test.shape)\n",
    "\n",
    "# print the first 1000 features (unsorted) of the count_vectorizer\n",
    "print(type(count_vectorizer.get_feature_names()))\n",
    "print(len(count_vectorizer.get_feature_names()))\n",
    "print(count_vectorizer.get_feature_names()[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '008704050406', '0121', '0125698789', '02', '0207', '02072069400', '02073162414', '02085076972', '021', '03', '04', '0430', '05', '050703', '0578', '06', '07', '07008009200', '07046744435', '07090298926', '07099833605', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '0776xxxxxxx', '07781482378', '07786200117', '077xxx', '078', '07801543489', '07808247860', '07808726822', '07815296484', '07821230901', '07880867867', '0789xxxxxxx', '07946746291', '07973788240', '07xxxxxxxxx', '08', '0800', '08000407165', '08000776320', '08000839402', '08000930705', '08000938767', '08001950382', '08002888812', '08002986030', '08002986906', '08006344447', '0808', '08081560665', '0825', '083', '0844', '08448714184', '0845', '08452810071', '08452810073', '08452810075over18', '0870', '08700435505150p', '08700469649', '08700621170150p', '08701213186', '08701237397', '08701417012', '08701417012150p', '0870141701216', '087016248', '08701752560', '087018728737', '0870241182716', '08702840625', '08704439680ts', '08706091795', '08707500020', '08707509020', '08707808226', '08708034412', '08708800282', '08709501522', '0871', '087104711148', '08712101358', '08712103738', '0871212025016', '08712300220', '08712317606', '08712400200', '08712400602450p', '08712400603', '08712402050', '08712402578', '08712402779', '08712402972', '08712405020', '08712405022', '08712460324', '08712466669', '0871277810710p', '0871277810810', '08714342399', '087147123779am', '08714712379', '08714712388', '08714712412', '08714714011', '08715203028', '08715203649', '08715203685', '08715203694', '08715705022', '0871750', '08717509990', '08717890890å', '08717895698', '08717898035', '08718720201', '08718723815', '08718725756', '08718726270', '087187262701', '08718726971', '08718726978', '08718727868', '08718727870', '08718727870150ppm', '08718730666', '08718738001', '08718738034', '08719180248', '08719181259', '08719181503', '08719181513', '08719839835', '08719899229', '08719899230', '09', '09041940223', '09050000332', '09050000460', '09050000555', '09050000928', '09050001295', '09050001808', '09050002311', '09050003091', '09050005321', '09050090044', '09050280520', '09053750005', '09056242159', '09057039994', '09058091854', '09058094455', '09058094565', '09058094583', '09058094594', '09058094597', '09058094599', '09058095107', '09058095201', '09058097189', '09058099801', '09061104283', '09061209465', '09061213237', '09061221061', '09061221066', '09061701444', '09061701461', '09061701851', '09061701939', '09061702893', '09061743386', '09061743806', '09061743810', '09061743811', '09061744553', '09061749602', '09061790121', '09061790126', '09063440451', '09063458130', '0906346330', '09064011000', '09064012103', '09064012160', '09064015307', '09064017295', '09064017305', '09064018838', '09064019014', '09065171142', '09065174042', '09065989182', '09066350750', '09066358152', '09066361921', '09066362206', '09066362220', '09066362231', '09066364311', '09066364349', '09066364589', '09066368327', '09066368753', '09066380611', '09066612661', '09066649731from', '09066660100', '09071512433', '09071517866', '09090204448', '09094100151', '09094646631', '09094646899', '09095350301', '09096102316', '09099726395', '09099726553', '09111030116', '09111032124', '09701213186', '0a', '0quit', '10', '100', '1000', '1000s', '100p', '100percent', '1013', '1030', '10am', '10k', '10p', '10ppm', '10th', '11', '113', '1131', '114', '116', '118p', '11mths', '11pm', '12', '120p', '121', '1225', '123', '1250', '125gift', '128', '12hours', '12hrs', '12mths', '13', '130', '1327', '139', '14', '1405', '140ppm', '145', '1450', '15', '150', '1500', '150p', '150p16', '150pm', '150ppermesssubscription', '150ppm', '150ppmpobox10183bhamb64xe', '150ppmsg', '150pw', '151', '15541', '15pm', '16', '165', '1680', '169', '177', '18', '1843', '18p', '18yrs', '195', '1956669', '1apple', '1b6a5ecef91ff9', '1cup', '1da', '1er', '1im', '1lemon', '1mega', '1st', '1st4terms', '1stchoice', '1stone', '1thing', '1tulsi', '1win150ppmx3', '1winaweek', '1winawk', '1x150p', '1yf', '20', '200', '2000', '2003', '2004', '2005', '2006', '2007', '200p', '2025050', '20p', '21', '21870000', '21st', '220', '220cm2', '2309', '23f', '23g', '24', '24hrs', '24m', '24th', '25', '250', '250k', '255', '25p', '26', '27', '28', '2814032', '28days', '28th', '28thfeb', '29', '2b', '2bold', '2day', '2ez', '2find', '2getha', '2geva', '2go', '2lands', '2marrow', '2moro', '2morow', '2morrow', '2mrw', '2nd', '2nhite', '2nights', '2nite', '2optout', '2px', '2rcv', '2stop', '2stoptxt', '2u', '2waxsto', '2wks', '2wu', '2years', '2yr', '2yrs', '30', '300', '3000', '300p', '3030', '30ish', '30pm', '30pp', '31', '3100', '310303', '32', '32000', '3230', '32323', '326', '33', '350', '3510i', '35p', '36504', '3680', '373', '3750', '37819', '3aj', '3d', '3days', '3g', '3gbp', '3hrs', '3lp', '3mins', '3optical', '3pound', '3qxj9', '3rd', '3ss', '3uz', '3xå', '40', '400', '400mins', '402', '4041', '40411', '40533', '40gb', '40mph', '41685', '41782', '4217', '430', '434', '440', '4403ldnw1a7rw18', '44345', '447797706009', '447801259231', '449071512431', '45', '450', '450p', '450pw', '45239', '45pm', '47', '4742', '48', '4882', '49557', '4a', '4d', '4eva', '4few', '4fil', '4get', '4give', '4got', '4info', '4jx', '4msgs', '4mths', '4qf2', '4t', '4th', '4the', '4txt', '4u', '4utxt', '4w', '4ward', '4wrd', '4xx26', '4years', '50', '500', '5000', '505060', '50award', '50ea', '50gbp', '50p', '50perweeksub', '50perwksub', '50pm', '50pmmorefrommobile2bremoved', '50rcvd', '515', '5226', '526', '528', '530', '54', '542', '5digital', '5free', '5ish', '5k', '5min', '5mls', '5p', '5pm', '5th', '5wb', '5we', '5wq', '5years', '600', '6031', '6089', '60p', '61200', '61610', '62220cncl', '62468', '62735', '630', '63miles', '645', '6669', '674', '68866', '69101', '69669', '69696', '69698', '69855', '69866', '69911', '69969', '69988', '6hl', '6hrs', '6ish', '6missed', '6months', '6pm', '6th', '6times', '6wu', '6zf', '700', '71', '7250i', '730', '731', '75', '750', '7548', '7634', '7684', '77', '78', '786', '7876150ppm', '79', '7am', '7cfca1a', '7ish', '7pm', '7th', '7ws', '7zs', '80', '800', '80062', '8007', '80086', '80155', '80182', '8027', '80488', '80608', '8077', '80878', '81010', '81151', '81303', '81618', '82050', '820554ad0a1705572711', '82242', '82277', '82324', '83039', '83049', '83110', '83118', '83222', '83338', '83355', '83370', '83383', '83435', '83600', '83738', '84025', '84128', '84199', '84484', '85', '85023', '85069', '86021', '861', '864233', '86688', '86888', '87021', '87066', '87077', '87121', '87131', '872', '87239', '87575', '88039', '88066', '88088', '88222', '88600', '8883', '88877', '88888', '89080', '89105', '89123', '89545', '89555', '89693', '89938', '8am', '8ball', '8pm', '8th', '8wp', '900', '9061100010', '9280114', '930', '9307622', '9755', '9758', '97n7qp', '98321561', '99', '9ae', '9am', '9ja', '9pm', '9t', '9th', '9yt', '____', 'a21', 'a30', 'aah', 'aaooooright', 'aathi', 'ab', 'abbey', 'abeg', 'abel', 'abi', 'ability', 'abiola', 'abj', 'able', 'abnormally', 'aboutas', 'abroad', 'absolutely', 'absolutly', 'abstract', 'abt', 'abta', 'aburo', 'abusers', 'ac', 'academic', 'acc', 'accent', 'accenture', 'accept', 'access', 'accessible', 'accidant', 'accident', 'accidentally', 'accommodation', 'accommodationvouchers', 'accomodate', 'accomodations', 'accordin', 'accordingly', 'account', 'accounting', 'accounts', 'accumulation', 'ache', 'acid', 'acl03530150pm', 'acnt', 'aco', 'acted', 'actin', 'action', 'activate', 'active', 'activities', 'actor', 'actual', 'actually', 'ad', 'adam', 'add', 'addamsfa', 'added', 'addicted', 'addie', 'adding', 'address', 'adds', 'adi', 'adjustable', 'admin', 'administrator', 'admirer', 'admission', 'adore', 'adoring', 'adress', 'ads', 'adsense', 'adult', 'adults', 'advance', 'adventure', 'advice', 'advise', 'advisors', 'affair', 'affairs', 'affection', 'affectionate', 'affections', 'affidavit', 'afford', 'afghanistan', 'afraid', 'african', 'aft', 'afternoon', 'afternoons', 'aftr', 'ag', 'agalla', 'age', 'age16', 'age23', 'agency', 'agent', 'agents', 'ages', 'aging', 'ago', 'agree', 'ah', 'aha', 'ahead', 'ahhh', 'ahmad', 'ahold', 'aid', 'aids', 'aig', 'aight', 'ain', 'aint', 'air', 'air1', 'airport', 'airtel', 'aiya', 'aiyah', 'aiyar', 'aiyo', 'ajith', 'ak', 'aka', 'al', 'alaipayuthe', 'album', 'alcohol', 'aldrine', 'alert', 'alerts', 'alex', 'alfie', 'algarve', 'algebra', 'algorithms', 'ali', 'alian', 'alibi', 'alive', 'allah', 'allalo', 'alle', 'allow', 'allowed', 'allows', 'alright', 'alrite', 'alter', 'alternative', 'alto18', 'aluable', 'alwa', 'alwys', 'amazing', 'ambitious', 'ambrith', 'american', 'ami', 'amigos', 'amk', 'amla', 'ammo', 'amore', 'amp', 'amplikater', 'amrca', 'amt', 'amused', 'amy', 'ana', 'analysis', 'anderson', 'andres', 'andros', 'angels', 'angry', 'animal', 'animation', 'anjie', 'anjola', 'anna', 'annie', 'anniversary', 'annoncement', 'announced', 'announcement', 'anot', 'ans', 'ansr', 'answer', 'answered', 'answerin', 'answering', 'answers', 'answr', 'antelope', 'antha', 'anthony', 'anti', 'anybody', 'anymore', 'anyones', 'anyplaces', 'anythiing', 'anythin', 'anythingtomorrow', 'anytime', 'anyways', 'aom', 'apart', 'apartment', 'apeshit', 'aphexåõs', 'apnt', 'apo', 'apologetic', 'apologise', 'app', 'apparently', 'appendix', 'applausestore', 'applebees', 'apples', 'application', 'apply', 'applying', 'appointments', 'appreciate', 'appreciated', 'approaches', 'approaching', 'appropriate', 'approved', 'approx', 'apps', 'appt', 'appy', 'april', 'apt', 'aquarius', 'ar', 'arab', 'arabian', 'archive', 'ard', 'area', 'aren', 'arent', 'aretaking', 'argh', 'argue', 'arguing', 'argument', 'aries', 'arm', 'armand', 'armenia', 'arms', 'arng', 'arnt', 'arrange', 'arrested', 'arrival', 'arrive', 'arrived', 'arrow', 'arsenal', 'art', 'artists', 'arts', 'arty', 'arul', 'arun', 'asa', 'asap', 'asda', 'ashes', 'ashwini', 'asia', 'asian', 'ask', 'askd', 'asked', 'askin', 'asking', 'asks', 'asleep', 'asp', 'ass', 'assessment', 'assistance', 'assume', 'assumed', 'asthere', 'astne', 'astrology', 'astronomer', 'asus', 'asusual', 'ate', 'atlanta', 'atlast', 'atm', 'atrocious', 'attach', 'attached', 'attempt', 'atten', 'attend', 'attended', 'attending', 'attention', 'attitude', 'attraction', 'attractive', 'attracts', 'atural', 'auction', 'audition', 'audrey', 'audrie', 'august', 'aunt', 'auntie', 'aunties', 'aunts', 'aunty', 'aust', 'australia']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# transform the training\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# transform the test\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 1000 features - same result as bow of course\n",
    "print(tfidf_vectorizer.get_feature_names()[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3733, 6782)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 vectors of the tfidf train using slicing on the .A (array) attrubute of tfidf_train\n",
    "\n",
    "print(type(tfidf_train.A))\n",
    "print(tfidf_train.A.shape)\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  008704050406  0121  0125698789  02  0207  02072069400  \\\n",
      "0   0    0             0     0           0   0     0            0   \n",
      "1   0    0             0     0           0   0     0            0   \n",
      "2   0    0             0     0           0   0     0            0   \n",
      "3   0    0             0     0           0   0     0            0   \n",
      "4   0    0             0     0           0   0     0            0   \n",
      "\n",
      "   02073162414  02085076972   ...    åòit  åômorrow  ì_  ìï  ó_  û_  ûªve  \\\n",
      "0            0            0   ...       0         0   0   0   0   0     0   \n",
      "1            0            0   ...       0         0   0   0   0   0     0   \n",
      "2            0            0   ...       0         0   0   0   0   0     0   \n",
      "3            0            0   ...       0         0   0   0   0   0     0   \n",
      "4            0            0   ...       0         0   0   0   0   0     0   \n",
      "\n",
      "   ûïharry  ûò  ûówell  \n",
      "0        0   0       0  \n",
      "1        0   0       0  \n",
      "2        0   0       0  \n",
      "3        0   0       0  \n",
      "4        0   0       0  \n",
      "\n",
      "[5 rows x 6782 columns]\n",
      "    00  000  008704050406  0121  0125698789   02  0207  02072069400  \\\n",
      "0  0.0  0.0           0.0   0.0         0.0  0.0   0.0          0.0   \n",
      "1  0.0  0.0           0.0   0.0         0.0  0.0   0.0          0.0   \n",
      "2  0.0  0.0           0.0   0.0         0.0  0.0   0.0          0.0   \n",
      "3  0.0  0.0           0.0   0.0         0.0  0.0   0.0          0.0   \n",
      "4  0.0  0.0           0.0   0.0         0.0  0.0   0.0          0.0   \n",
      "\n",
      "   02073162414  02085076972   ...    åòit  åômorrow   ì_   ìï   ó_   û_  ûªve  \\\n",
      "0          0.0          0.0   ...     0.0       0.0  0.0  0.0  0.0  0.0   0.0   \n",
      "1          0.0          0.0   ...     0.0       0.0  0.0  0.0  0.0  0.0   0.0   \n",
      "2          0.0          0.0   ...     0.0       0.0  0.0  0.0  0.0  0.0   0.0   \n",
      "3          0.0          0.0   ...     0.0       0.0  0.0  0.0  0.0  0.0   0.0   \n",
      "4          0.0          0.0   ...     0.0       0.0  0.0  0.0  0.0  0.0   0.0   \n",
      "\n",
      "   ûïharry   ûò  ûówell  \n",
      "0      0.0  0.0     0.0  \n",
      "1      0.0  0.0     0.0  \n",
      "2      0.0  0.0     0.0  \n",
      "3      0.0  0.0     0.0  \n",
      "4      0.0  0.0     0.0  \n",
      "\n",
      "[5 rows x 6782 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the bow and the tf-idf into pandas\n",
    "\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "print(count_df.head(5))\n",
    "\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print(tfidf_df.head(5))\n",
    "\n",
    "\n",
    "\n",
    "# test if the column names are the same in both dfs\n",
    "difference = set(count_df.columns)-set(tfidf_df.columns)\n",
    "\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9847743338771071"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 220,   21],\n",
       "       [   7, 1591]], dtype=int64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, pred, labels=[\"spam\",\"ham\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128630705394191"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test, pred, pos_label=\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691629955947136"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test, pred, pos_label=\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662860250135944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 179,   62],\n",
       "       [   0, 1598]], dtype=int64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, pred))\n",
    "\n",
    "metrics.confusion_matrix(y_test, pred, labels=[\"spam\",\"ham\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision\n",
    "# recall\n",
    "# imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n",
      "Alpha:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9760739532354541\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9815116911364872\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.9820554649265906\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.9825992387166939\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.9804241435562806\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.9798803697661773\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.9766177270255574\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.9733550842849374\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.9717237629146275\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.9684611201740077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination.\n",
    "import numpy as np\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "# Values should range from 0 to 1 with steps of 0.1.\n",
    "alphas = np.arange(0,1,0.1)\n",
    "print(type(alphas))\n",
    "print(alphas)\n",
    "\n",
    "\n",
    "# Define train_and_predict() that is a function of alpha\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier classifier with alpha=alpha :nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n",
      "<class 'list'>\n",
      "6782\n",
      "['00', '000', '008704050406', '0121', '0125698789', '02', '0207', '02072069400', '02073162414', '02085076972']\n",
      "ham [(-9.06709846527481, '0125698789'), (-9.06709846527481, '0quit'), (-9.06709846527481, '1030'), (-9.06709846527481, '116'), (-9.06709846527481, '128'), (-9.06709846527481, '130'), (-9.06709846527481, '1405'), (-9.06709846527481, '15'), (-9.06709846527481, '15pm'), (-9.06709846527481, '1680'), (-9.06709846527481, '1843'), (-9.06709846527481, '1apple'), (-9.06709846527481, '1cup'), (-9.06709846527481, '1im'), (-9.06709846527481, '1lemon'), (-9.06709846527481, '1mega'), (-9.06709846527481, '1stone'), (-9.06709846527481, '1thing'), (-9.06709846527481, '1tulsi'), (-9.06709846527481, '24th')]\n",
      "----------------\n",
      "spam [(-6.90031675326269, 'win'), (-6.849960098045868, 'guaranteed'), (-6.768838150141869, '150p'), (-6.741445206454991, 'contact'), (-6.738590915577634, 'new'), (-6.737871538928443, 'urgent'), (-6.682180685636441, 'service'), (-6.663878826411415, 'won'), (-6.630580995071538, 'uk'), (-6.626263508369291, 'cash'), (-6.592863346362673, 'www'), (-6.487697527509885, 'reply'), (-6.4404237584395485, 'ur'), (-6.422123387940722, 'text'), (-6.391217432144195, 'stop'), (-6.371540358776016, 'prize'), (-6.34946499189957, 'claim'), (-6.250403231787892, 'mobile'), (-6.246730562794488, 'txt'), (-5.954161890519181, 'free')]\n"
     ]
    }
   ],
   "source": [
    "# Save the class labels using the .classes_ attribute of nb_classifier: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "print(class_labels)\n",
    "\n",
    "# Extract the features: feature_names - A list with all the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(type(feature_names))\n",
    "print(len(feature_names))\n",
    "print(feature_names[0:10])\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights\n",
    "#Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first use zip() with the arguments nb_classifier.coef_[0] and feature_names. Then, use sorted() on this.\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.8666666666666667, 'Boris Johnson')],\n",
       " [(0.8333333333333334, 'Diane Abbott')],\n",
       " None,\n",
       " [(0.3846153846153846, 'Boris Johnson')],\n",
       " [(0.5384615384615384, 'Boris Johnson')])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fuzzyset\n",
    " \n",
    "fz = fuzzyset.FuzzySet()\n",
    "#Create a list of terms we would like to match against in a fuzzy way\n",
    "for l in [\"Diane Abbott\", \"Boris Johnson\"]:\n",
    "    fz.add(l)\n",
    " \n",
    "#Now see if our sample term fuzzy matches any of those specified terms\n",
    "sample_term='Boris Johnstone'\n",
    "fz.get(sample_term), fz.get('Diana Abbot'), fz.get('Oz'), fz.get(\"Boris\"), fz.get(\"Boris o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
